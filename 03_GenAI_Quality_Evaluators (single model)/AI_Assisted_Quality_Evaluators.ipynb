{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Goal of the Exercise  \n",
    "\n",
    "In this exercise, you'll learn how to evaluate the **quality of AI-generated text responses** using advanced, AI-assisted evaluators such as **Relevance**, **Coherence**, **Fluency**, **Groundedness**, and custom-built evaluators. Your tasks involve implementing, configuring, and applying these evaluators to ensure generated content meets high-quality standards.\n",
    "\n",
    "Through this exercise, you'll gain practical experience in:\n",
    "\n",
    "- Identifying the key dimensions of text quality (relevance, coherence, fluency, groundedness).\n",
    "- Leveraging AI-assisted built-in evaluators (*LLM-as-a-judge*) to assess text outputs.\n",
    "- Creating custom evaluators tailored to your specific use cases or quality standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install azure-ai-evaluation\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Response Quality with RelevanceEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f224e40",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the Relevance Evaluator\n",
    "\n",
    "Evaluates relevance score for a given query and response or a multi-turn conversation, including reasoning.\n",
    "\n",
    "The relevance measure assesses the ability of answers to capture the key points of the context. High relevance scores signify the AI system's understanding of the input and its capability to produce coherent and contextually appropriate outputs. Conversely, low relevance scores indicate that generated responses might be off-topic, lacking in context, or insufficient in addressing the user's intended queries. Use the relevance metric when evaluating the AI system's performance in understanding the input and generating contextually appropriate responses.\n",
    "\n",
    "Relevance scores range from 1 to 5, with 1 being the worst and 5 being the best.\n",
    "\n",
    "Fill in the missing code to initialize and use the `RelevanceEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run the RelevanceEvaluator in order to eveluate\n",
    "# the relevance of a response compared to the initial query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Response Quality with CoherenceEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457e717",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the Coherence Evaluator\n",
    "\n",
    "Evaluates coherence score for a given query and response or a multi-turn conversation, including reasoning.\n",
    "\n",
    "The coherence measure assesses the ability of the language model to generate text that reads naturally, flows smoothly, and resembles human-like language in its responses. Use it when assessing the readability and user-friendliness of a model's generated responses in real-world applications.\n",
    "\n",
    "Complete the code below to instantiate and use the `CoherenceEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run the CoherenceEvaluator in order to evaluate\n",
    "# the coherence of a response compared to the initial query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Response Quality with FluencyEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9c4de",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the Fluency Evaluator\n",
    "\n",
    "Evaluates the fluency of a given response or a multi-turn conversation, including reasoning.\n",
    "\n",
    "The fluency measure assesses the extent to which the generated text conforms to grammatical rules, syntactic structures, and appropriate vocabulary usage, resulting in linguistically correct responses.\n",
    "\n",
    "Fluency scores range from 1 to 5, with 1 being the least fluent and 5 being the most fluent.\n",
    "\n",
    "Fill in the missing code to initialize and use the `FluencyEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run the FluencyEvaluator in order to evaluate\n",
    "# the fluency of a response from a grammatical, syntactic and appropriate vocabulary usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6b81d",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the Groundedness Evaluator\n",
    "\n",
    "Evaluates groundedness score for a given query (optional), response, and context or a multi-turn conversation, including reasoning.\n",
    "\n",
    "The groundedness measure assesses the correspondence between claims in an AI-generated answer and the source context, making sure that these claims are substantiated by the context. Even if the responses from LLM are factually correct, they'll be considered ungrounded if they can't be verified against the provided sources (such as your input source or your database). Use the groundedness metric when you need to verify that AI-generated responses align with and are validated by the provided context.\n",
    "\n",
    "Groundedness scores range from 1 to 5, with 1 being the least grounded and 5 being the most grounded.\n",
    "\n",
    "Complete the code below to instantiate and use the `GroundednessEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run the GroundednessEvaluator in order to evaluate\n",
    "# the groundedness of a response compared to the retrieved context which is useful in RAG scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating custom evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code-based evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function-based evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bec496",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement a Custom Function-based Evaluator\n",
    "Write a function to measure the length of a response.\n",
    "\n",
    "**At this stage, it doesn't require to involve an LLM to assist you with this evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run a function-based evaluator\n",
    "# Example \"Custom evaluator function to calculate response length\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-based evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0255f5",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement a Custom Class-based Evaluator\n",
    "Create a class-based evaluator that checks responses for blocked words.\n",
    "\n",
    "**At this stage, it doesn't require to involve an LLM to assist you with this evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run a class-based evaluator\n",
    "# Example Custom class-based evaluator to check for blocked words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt-based evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpfulness evaluator\n",
    "\n",
    "As you can find in this folder, a custom evaluator named **\"helpfulness\"** has been created. Its purpose is evaluate, using an LLM, how much *\"helpful\"* is a given response.\n",
    "\n",
    "You'll find 2 files:\n",
    " - ```helpfulness.prompty``` is a prompty file that aims to templatize your prompts, specifying model, hyperparameters, instructions, etc. This is here that we insert our instructions for the custom evaluator.\n",
    " - ```helpfulness.py``` is a python module that we use to create the **HelpfulnessEvaluator** class and be callable from the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15cca6a",
   "metadata": {},
   "source": [
    "### ðŸ‘€ Observe: Using the custom Helpfulness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpfulness import HelpfulnessEvaluator\n",
    "\n",
    "helpfulness_evaluator = HelpfulnessEvaluator(model_config)\n",
    "\n",
    "helpfulness_score = helpfulness_evaluator(\n",
    "    query=\"What's the meaning of life?\", \n",
    "    context=\"Arthur Schopenhauer was the first to explicitly ask the question, in an essay entitled 'Character'.\", \n",
    "    response=\"The answer is 42.\"\n",
    ")\n",
    "print(helpfulness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON accuracy evaluator\n",
    "\n",
    "Based on the HelpfulnessEvaluatore, implement your own custom evaluator.\n",
    "\n",
    "**Idea**: create a JSON Schema evaluator. Goal of this custom evaluator is to evaluate how much a JSON output complies to a given schema. \n",
    "\n",
    "**jsons**: you'll find a ```jsons``` folder that contains some jsons outputs (```complete/poor/very_poor_output.json```) and a json schema file (```example_schema.json```). You can leverage these files to perform the evaluation and compare scores and reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c69f4",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement a JSON Schema Evaluator\n",
    "Load a JSON schema and use it to evaluate JSON objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement JSON schema evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf54a19",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Evaluate the dataset with built-in and custom evaluators\n",
    "Fill in the missing code to initialize and different evaluators such as built-in (`RelevanceEvaluator`, `CoherenceEvaluator`, `FluencyEvaluator`, `GroundednessEvaluator`, `RetrievalEvaluator`, etc.) and custom ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate, configure and run the different evaluators. \n",
    "# You can use the model_endpoint module as the target to interact with the model endpoint and get the output to evaluate.\n",
    "# Example: RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, GroundednessEvaluator, HelpfulnessEvaluator\n",
    "# You'll need to configure the evaluators with the appropriate column_mapping and run them on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display results dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
