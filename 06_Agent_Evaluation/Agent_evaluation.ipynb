{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6ab610",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Goal of the Exercise  \n",
    "\n",
    "This notebook demonstrates how to evaluate AI agents using the Azure AI Foundry Agent Evaluation SDK.\n",
    "\n",
    "Agent evaluation is a crucial step in the development of AI systems, ensuring that agents behave as expected and meet performance requirements in various scenarios. Azure AI Foundry provides a robust SDK that simplifies the process of defining evaluation tasks, running them against one or more agents, and analyzing the results.\n",
    "\n",
    "In this notebook, we walk through the following key components:\n",
    "\n",
    "Setting up and configuring the evaluation environment\n",
    "Defining custom evaluators and datasets\n",
    "Running evaluations on different agent implementations\n",
    "Analyzing the outputs to gain insights into agent performance\n",
    "\n",
    "By the end of this notebook, you'll have a clear understanding of how to use the Agent Evaluation SDK to test and validate your AI agents in a structured, repeatable manner.\n",
    "\n",
    "# Links to documentation\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/agent-evaluate-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33a09b",
   "metadata": {},
   "source": [
    "### Create a custom Python function and register it in the Toolset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82de94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.agents.models import FunctionTool, ToolSet\n",
    "from typing import Set, Callable, Any\n",
    "import json\n",
    "\n",
    "# TODO: Define a custom Python function on weather or reuse the one created in the previous exercise.\n",
    "# TODO: Add tools that the agent will use. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a087b31",
   "metadata": {},
   "source": [
    "### Create the agent, iniate the thread, add message and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# TODO : Create an AIProjectClient instance\n",
    "# TODO : Create an agent with the toolset \n",
    "# TODO : Enable auto function calls for the agent\n",
    "# TODO : Create a thread for communication\n",
    "# TODO : Add a message to the thread\n",
    "# TODO : Create and process an agent run\n",
    "# TODO : Check if the run failed\n",
    "# TODO : Fetch and log all messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0300f3",
   "metadata": {},
   "source": [
    "### Get the converted data from the run/thread id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d73c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# TODO : Initialize the converter for Azure AI agents.\n",
    "# TODO : Specify the thread and run ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442daafb",
   "metadata": {},
   "source": [
    "### Evaluate a single agent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fa05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import IntentResolutionEvaluator, TaskAdherenceEvaluator, ToolCallAccuracyEvaluator\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# TODO : Initialize model_config\n",
    "# TODO : Evaluators with standard model support\n",
    "# TODO : Reference the quality evaluators list above.\n",
    "# TODO : Leverage each evaluator with converted_data and print the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9dde0d",
   "metadata": {},
   "source": [
    "### OPTIONAL : Evaluate multiple agent runs or threads\n",
    "\n",
    "First, convert your agent thread data into a file via our converter support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8959b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO : Specify a file path to save the agent output (evaluation input data) and print the file path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad761c9",
   "metadata": {},
   "source": [
    "Leverage the Batch evaluate API for asynchronous evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02041e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    IntentResolutionEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# TODO : Initialize each evaluator with the model_config\n",
    "# TODO : Evaluate the evaluation dataset file with the evaluators, register it to the Foundry project and print the results and the URL to the AI Foundry project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiops2envagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
