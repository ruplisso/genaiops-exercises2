{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Goal of the Exercise  \n",
    "\n",
    "In this exercise, you'll explore how to leverage AI evaluators to effectively assess **safety risks** within text content, such as **violence**, **self-harm** but also **indirect attack (XPIA)**. \n",
    "Your tasks include implementing and configuring evaluators that accurately detect and measure potentially safety risks content.\n",
    "\n",
    "Through this activity, you'll learn:\n",
    "\n",
    "- How to use specialized evaluators for detecting sensitive or risky language.\n",
    "- Best practices for integrating safety checks into AI workflows.\n",
    "- Techniques for interpreting and acting upon evaluator results to maintain ethical and responsible AI systems.\n",
    "\n",
    "# Links to documentation\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/risk-safety-evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing safety with ViolenceEvaluator\n",
    "\n",
    "Evaluates the violence score for a given query and response or a multi-turn conversation.\n",
    "\n",
    "Violent content includes language pertaining to physical actions intended to hurt, injure, damage, or kill someone or something. It also includes descriptions of weapons and guns (and related entities such as manufacturers and associations).\n",
    "\n",
    "Safety evaluations annotate self-harm-related content using a 0-7 scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate and configure the ViolenceEvaluator and use it to evaluate content.\n",
    "# Try different content types and see how the evaluator responds in terms of scores and reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing safety with SelfHarmEvaluator\n",
    "\n",
    "Evaluates self-harm score for a given query and response or a multi-turn conversation.\n",
    "\n",
    "Self-harm-related content includes language pertaining to actions intended to hurt, injure, or damage one's body or kill oneself.\n",
    "\n",
    "Safety evaluations annotate self-harm-related content using a 0-7 scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replicate the evaluation process above using the SelfHarmEvaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing safety with IndirectAttackEvaluator\n",
    "\n",
    "Evaluates the indirect attack score for a given query and response or a multi-turn conversation, with reasoning.\n",
    "\n",
    "Indirect attacks, also known as cross-domain prompt injected attacks (XPIA), are when jailbreak attacks are injected into the context of a document or source that may result in an altered, unexpected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replicate the evaluation process above using the IndirectAttackEvaluator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
