{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95b03d1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/agent-evaluate-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb15c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.agents.models import FunctionTool\n",
    "from azure.ai.evaluation import AIAgentConverter, IntentResolutionEvaluator\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "# Create a fake fetch_weather function\n",
    "def fetch_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the weather information for the specified location.\n",
    "\n",
    "    :param location: The location to fetch weather for.\n",
    "    :return: Weather information as a JSON string.\n",
    "    \"\"\"\n",
    "    # Mock weather data for demonstration purposes\n",
    "    mock_weather_data = {\"New York\": \"Sunny, 25°C\", \"London\": \"Cloudy, 18°C\", \"Tokyo\": \"Rainy, 22°C\"}\n",
    "    weather = mock_weather_data.get(location, \"Weather data not available for this location.\")\n",
    "    return json.dumps({\"weather\": weather})\n",
    "\n",
    "# Define user functions\n",
    "user_functions = {fetch_weather}\n",
    "\n",
    "# Retrieve the project endpoint from environment variables\n",
    "project_endpoint = \"https://projetagent-resource.services.ai.azure.com/api/projects/projetagent\"\n",
    "\n",
    "# Initialize the AIProjectClient\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=project_endpoint,\n",
    "    credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "# Initialize the FunctionTool with user-defined functions\n",
    "functions = FunctionTool(functions=user_functions)\n",
    "\n",
    "with project_client:\n",
    "    \n",
    "    # Create an agent with custom functions\n",
    "    agent = project_client.agents.create_agent(\n",
    "        model=\"gpt-4o\",\n",
    "        name=\"my-agent-to-be-evaluated\",\n",
    "        instructions=\"You are a helpful agent\",\n",
    "        tools=functions.definitions,\n",
    "    )\n",
    "    print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "    # Create a thread for communication\n",
    "    thread = project_client.agents.threads.create()\n",
    "    print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "    # Send a message to the thread\n",
    "    message = project_client.agents.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=\"Hello, what about the weather in New York?\",\n",
    "    )\n",
    "    print(f\"Created message, ID: {message['id']}\")\n",
    "\n",
    "    run = project_client.agents.runs.create(thread_id=thread.id, agent_id=agent.id)\n",
    "    print(f\"Created run, ID: {run.id}\")\n",
    "\n",
    "    # Poll the run status until it is completed or requires action\n",
    "    while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n",
    "        time.sleep(1)\n",
    "        run = project_client.agents.runs.get(thread_id=thread.id, run_id=run.id)\n",
    "\n",
    "        if run.status == \"requires_action\":\n",
    "            tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "            tool_outputs = []\n",
    "            for tool_call in tool_calls:\n",
    "                if tool_call.function.name == \"fetch_weather\":\n",
    "                    output = fetch_weather(\"New York\")\n",
    "                    tool_outputs.append({\"tool_call_id\": tool_call.id, \"output\": output})\n",
    "            project_client.agents.runs.submit_tool_outputs(thread_id=thread.id, run_id=run.id, tool_outputs=tool_outputs)\n",
    "\n",
    "    print(f\"Run completed with status: {run.status}\")\n",
    "\n",
    "    # Fetch and log all messages from the thread\n",
    "    messages = project_client.agents.messages.list(thread_id=thread.id,order=\"asc\")\n",
    "    for message in messages:\n",
    "        print(f\"Role: {message['role']}\")\n",
    "        print(f\"Content: {message['content']}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "    # Initialize the converter for Azure AI agents.\n",
    "    converter = AIAgentConverter(project_client)\n",
    "\n",
    "    # Specify the thread and run ID.\n",
    "    thread_id = thread.id\n",
    "    run_id = run.id\n",
    "    converted_data = converter.convert(thread_id, run_id)\n",
    "\n",
    "    # Specify a file path to save the agent output (evaluation input data) to.\n",
    "    filename = os.path.join(os.getcwd(), \"evaluation_input_data.jsonl\")\n",
    "    evaluation_data = converter.prepare_evaluation_data(thread_ids=thread_id, filename=filename) \n",
    "    print(f\"Evaluation data saved to {filename}\")\n",
    "\n",
    "    # Delete the agent after use\n",
    "    project_client.agents.delete_agent(agent.id)\n",
    "    print(\"Deleted agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cecc59b",
   "metadata": {},
   "source": [
    "Display Converted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e41da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa1ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is specific to agentic workflows.\n",
    "from azure.ai.evaluation import IntentResolutionEvaluator, TaskAdherenceEvaluator, ToolCallAccuracyEvaluator\n",
    "\n",
    "# Other quality, risk, and safety metrics:\n",
    "from azure.ai.evaluation import RelevanceEvaluator, CoherenceEvaluator, CodeVulnerabilityEvaluator, ContentSafetyEvaluator, IndirectAttackEvaluator, FluencyEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_config = {\n",
    "    \"azure_deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}\n",
    "\n",
    "reasoning_model_config = {\n",
    "    \"azure_deployment\": \"o4-mini\",\n",
    "    \"api_key\": os.getenv(\"AZURE_API_KEY\"),\n",
    "    \"azure_endpoint\": os.getenv(\"AZURE_ENDPOINT\"),\n",
    "    \"api_version\": os.getenv(\"AZURE_API_VERSION\"),\n",
    "}\n",
    "\n",
    "# Evaluators with reasoning model support\n",
    "quality_evaluators = {evaluator.__name__: evaluator(model_config=model_config) for evaluator in [IntentResolutionEvaluator, TaskAdherenceEvaluator, ToolCallAccuracyEvaluator]}\n",
    "\n",
    "## Using Azure AI Foundry (non-Hub) project endpoint, example: AZURE_AI_PROJECT=https://your-account.services.ai.azure.com/api/projects/your-project\n",
    "\n",
    "azure_ai_project = \"https://projetagent-resource.services.ai.azure.com/api/projects/projetagent\"\n",
    "\n",
    "# Reference the quality and safety evaluator list above.\n",
    "quality_and_safety_evaluators = {**quality_evaluators}\n",
    "\n",
    "for name, evaluator in quality_and_safety_evaluators.items():\n",
    "    result = evaluator(**converted_data)\n",
    "    print(name)\n",
    "    print(json.dumps(result, indent=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Batch evaluation API (local):\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    IntentResolutionEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)\n",
    "\n",
    "response = evaluate(\n",
    "    data=filename,\n",
    "    evaluation_name=\"agent demo - batch run\",\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    # optionally, log your results to your Azure AI Foundry project for rich visualization \n",
    "    azure_ai_project = \"https://projetagent-resource.services.ai.azure.com/api/projects/projetagent\", # example: https://your-account.services.ai.azure.com/api/projects/your-project\n",
    ")\n",
    "# Inspect the average scores at a high level.\n",
    "print(response[\"metrics\"])\n",
    "# Use the URL to inspect the results on the UI.\n",
    "print(f'AI Foundry URL: {response.get(\"studio_url\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937a350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiops2envagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
