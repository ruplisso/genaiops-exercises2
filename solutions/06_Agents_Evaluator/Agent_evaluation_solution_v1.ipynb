{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6ab610",
   "metadata": {},
   "source": [
    "https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/agent-evaluate-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33a09b",
   "metadata": {},
   "source": [
    "### Create a custom Python function and register it in the Toolset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e82de94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from azure.ai.agents.models import FunctionTool, ToolSet\n",
    "from typing import Set, Callable, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "weather_api_key = os.environ.get(\"WEATHER_API_KEY\")\n",
    "\n",
    "# Define a custom Python function.\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the weather information for the specified location.\n",
    "\n",
    "    :param location (str): The location to fetch weather for.\n",
    "    :return: Weather information as a JSON string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    url = f\"http://api.weatherapi.com/v1/current.json?key={weather_api_key}&q={city}&aqi=no\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        condition = data[\"current\"][\"condition\"][\"text\"]\n",
    "        temp_c = data[\"current\"][\"temp_c\"]\n",
    "        return {\"condition\": condition,\"temperature\": temp_c}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "    \n",
    "user_functions: Set[Callable[..., Any]] = {\n",
    "    get_weather,\n",
    "}\n",
    "\n",
    "# Add tools that the agent will use. \n",
    "functions = FunctionTool(user_functions)\n",
    "\n",
    "toolset = ToolSet()\n",
    "toolset.add(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a087b31",
   "metadata": {},
   "source": [
    "### Create the agent, iniate the thread, add message and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0387b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.ai.agents \n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "# Create an Azure AI Client from an endpoint, copied from your Azure AI Foundry project.\n",
    "project_endpoint = os.environ[\"AI_PROJECT_ENDPOINT\"]  # Ensure the PROJECT_ENDPOINT environment variable is set\n",
    "\n",
    "# Create an AIProjectClient instance\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=project_endpoint,\n",
    "    credential=DefaultAzureCredential(),  # Use Azure Default Credential for authentication\n",
    ")\n",
    "\n",
    "# Create an agent with the toolset \n",
    "agent = project_client.agents.create_agent(\n",
    "    model=\"gpt-4o\",  # Model deployment name\n",
    "    name=\"my-agent-to-be-evaluated\",  # Name of the agent\n",
    "    instructions=\"You are a helpful agent\",  # Instructions for the agent\n",
    "    toolset=toolset\n",
    ")\n",
    "print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "# Enable auto function calls for the agent\n",
    "project_client.agents.enable_auto_function_calls(toolset)\n",
    "\n",
    "# Create a thread for communication\n",
    "thread = project_client.agents.threads.create()\n",
    "print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "# Add a message to the thread\n",
    "message = project_client.agents.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",  # Role of the message sender\n",
    "    content=\"What is the weather in Tokyo today?\",  # Message content\n",
    ")\n",
    "print(f\"Created message, ID: {message['id']}\")\n",
    "\n",
    "# Create and process an agent run\n",
    "run = project_client.agents.runs.create_and_process(thread_id=thread.id, agent_id=agent.id)\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "# Check if the run failed\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "# Fetch and log all messages\n",
    "messages = project_client.agents.messages.list(thread_id=thread.id,order=\"asc\")\n",
    "for message in messages:\n",
    "    print(f\"Role: {message['role']}\")\n",
    "    print(f\"Content: {message['content']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0300f3",
   "metadata": {},
   "source": [
    "### Get the converted data from the run/thread id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65d73c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# Initialize the converter for Azure AI agents.\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "# Specify the thread and run ID.\n",
    "thread_id = thread.id\n",
    "run_id = run.id\n",
    "\n",
    "converted_data = converter.convert(thread_id, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442daafb",
   "metadata": {},
   "source": [
    "### Evaluate a single agent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de7fa05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is specific to agentic workflows.\n",
    "from azure.ai.evaluation import IntentResolutionEvaluator, TaskAdherenceEvaluator, ToolCallAccuracyEvaluator\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_config = {\n",
    "    \"azure_deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}\n",
    "\n",
    "# Evaluators with standard model support\n",
    "quality_evaluators = {evaluator.__name__: evaluator(model_config=model_config) for evaluator in [IntentResolutionEvaluator, TaskAdherenceEvaluator, ToolCallAccuracyEvaluator]}\n",
    "\n",
    "# Reference the quality and safety evaluator list above.\n",
    "quality_and_safety_evaluators = {**quality_evaluators}\n",
    "\n",
    "for name, evaluator in quality_and_safety_evaluators.items():\n",
    "    result = evaluator(**converted_data)\n",
    "    print(name)\n",
    "    print(json.dumps(result, indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9dde0d",
   "metadata": {},
   "source": [
    "### Evaluate multiple agent runs or threads\n",
    "\n",
    "First, convert your agent thread data into a file via our converter support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8959b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a file path to save the agent output (evaluation input data) to.\n",
    "filename = os.path.join(os.getcwd(), \"evaluation_input_data.jsonl\")\n",
    "\n",
    "evaluation_data = converter.prepare_evaluation_data(thread_ids=thread_id, filename=filename) \n",
    "\n",
    "print(f\"Evaluation data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad761c9",
   "metadata": {},
   "source": [
    "Leverage the Batch evaluate API for asynchronous evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02041e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent, ID: asst_72W0slNHmZUlo0rpoGDrjI5y\n",
      "Created thread, ID: thread_UzwYxcUKqf7GND5eYtl2Fd3H\n",
      "Created message, ID: msg_eLeoQ6QYU1Wfa3aEc0yEVN9l\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Role: user\n",
      "Content: [{'type': 'text', 'text': {'value': 'What is the weather in Tokyo today?', 'annotations': []}}]\n",
      "----------------------------------------\n",
      "Role: assistant\n",
      "Content: [{'type': 'text', 'text': {'value': 'The weather in Tokyo today is clear, with a temperature of 28.0Â°C.', 'annotations': []}}]\n",
      "----------------------------------------\n",
      "IntentResolutionEvaluator\n",
      "{\n",
      "    \"intent_resolution\": 5.0,\n",
      "    \"intent_resolution_result\": \"pass\",\n",
      "    \"intent_resolution_threshold\": 3,\n",
      "    \"intent_resolution_reason\": \"User wanted the current weather in Tokyo. Agent provided a clear, accurate response with temperature and conditions, fully resolving the intent without any gaps or issues.\"\n",
      "}\n",
      "TaskAdherenceEvaluator\n",
      "{\n",
      "    \"task_adherence\": 5.0,\n",
      "    \"task_adherence_result\": \"pass\",\n",
      "    \"task_adherence_threshold\": 3,\n",
      "    \"task_adherence_reason\": \"The assistant correctly identified the task, used the appropriate tool, and provided a clear and accurate weather update for Tokyo, fully satisfying the user's request.\"\n",
      "}\n",
      "ToolCallAccuracyEvaluator\n",
      "{\n",
      "    \"tool_call_accuracy\": 5.0,\n",
      "    \"tool_call_accuracy_result\": \"pass\",\n",
      "    \"tool_call_accuracy_threshold\": 3,\n",
      "    \"tool_call_accuracy_reason\": \"Let's think step by step: The user asked for the weather in Tokyo today. The agent correctly called the 'get_weather' tool with the parameter 'city' set to 'Tokyo', which is directly grounded in the user's query. The tool returned the weather information without any errors, and the agent did not make any unnecessary or excessive tool calls. No missing tool calls were required to address the query. Therefore, the tool call was fully relevant, efficient, and optimal in addressing the user's query.\",\n",
      "    \"details\": {\n",
      "        \"tool_calls_made_by_agent\": 1,\n",
      "        \"correct_tool_calls_made_by_agent\": 1,\n",
      "        \"per_tool_call_details\": [\n",
      "            {\n",
      "                \"tool_name\": \"get_weather\",\n",
      "                \"total_calls_required\": 1,\n",
      "                \"correct_calls_made_by_agent\": 1,\n",
      "                \"correct_tool_percentage\": 1.0,\n",
      "                \"tool_call_errors\": 0,\n",
      "                \"tool_success_result\": \"pass\"\n",
      "            }\n",
      "        ],\n",
      "        \"excess_tool_calls\": {\n",
      "            \"total\": 0,\n",
      "            \"details\": []\n",
      "        },\n",
      "        \"missing_tool_calls\": {\n",
      "            \"total\": 0,\n",
      "            \"details\": []\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Evaluation data saved to c:\\Users\\ruplisso\\Documents\\GitHub\\genaiops-exercises2\\solutions\\06_Agents_Evaluator\\evaluation_input_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    IntentResolutionEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "ai_project_endpoint=os.environ[\"AI_PROJECT_ENDPOINT\"]\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)\n",
    "\n",
    "response = evaluate(\n",
    "    data=filename,\n",
    "    evaluation_name=\"agent demo - batch run\",\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    # optionally, log your results to your Azure AI Foundry project for rich visualization \n",
    "    azure_ai_project = ai_project_endpoint\n",
    ")\n",
    "# Inspect the average scores at a high level.\n",
    "print(response[\"metrics\"])\n",
    "\n",
    "# Use the URL to inspect the results on the UI.\n",
    "print(f'AI Foundry URL: {response.get(\"studio_url\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiops2envagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
