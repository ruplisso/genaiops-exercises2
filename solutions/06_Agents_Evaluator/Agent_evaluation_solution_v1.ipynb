{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6ab610",
   "metadata": {},
   "source": [
    "https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/agent-evaluate-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33a09b",
   "metadata": {},
   "source": [
    "### Create a custom Python function and register it in the Toolset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82de94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from azure.ai.agents.models import FunctionTool, ToolSet\n",
    "from typing import Set, Callable, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "weather_api_key = os.environ.get(\"WEATHER_API_KEY\")\n",
    "\n",
    "# Define a custom Python function.\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the weather information for the specified location.\n",
    "\n",
    "    :param location (str): The location to fetch weather for.\n",
    "    :return: Weather information as a JSON string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    url = f\"http://api.weatherapi.com/v1/current.json?key={weather_api_key}&q={city}&aqi=no\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        condition = data[\"current\"][\"condition\"][\"text\"]\n",
    "        temp_c = data[\"current\"][\"temp_c\"]\n",
    "        return {\"condition\": condition,\"temperature\": temp_c}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "    \n",
    "user_functions: Set[Callable[..., Any]] = {\n",
    "    get_weather,\n",
    "}\n",
    "\n",
    "# Add tools that the agent will use. \n",
    "functions = FunctionTool(user_functions)\n",
    "\n",
    "toolset = ToolSet()\n",
    "toolset.add(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a087b31",
   "metadata": {},
   "source": [
    "### Create the agent, iniate the thread, add message and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0387b4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent, ID: asst_0FGW7zNAmjLwjtuovA34ePqR\n",
      "Created thread, ID: thread_7mEZjs7dVanly8MqiLnRSPSi\n",
      "Created message, ID: msg_5r5a7HSUuCkjuuBR20ishbpb\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Role: user\n",
      "Content: [{'type': 'text', 'text': {'value': 'What is the weather in Bordeaux today?', 'annotations': []}}]\n",
      "----------------------------------------\n",
      "Role: assistant\n",
      "Content: [{'type': 'text', 'text': {'value': 'The weather in Bordeaux today is moderately rainy with a temperature of 19.2Â°C.', 'annotations': []}}]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azure.ai.agents \n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "# Create an Azure AI Client from an endpoint, copied from your Azure AI Foundry project.\n",
    "project_endpoint = os.environ[\"AI_PROJECT_ENDPOINT\"]  # Ensure the PROJECT_ENDPOINT environment variable is set\n",
    "\n",
    "# Create an AIProjectClient instance\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=project_endpoint,\n",
    "    credential=DefaultAzureCredential(),  # Use Azure Default Credential for authentication\n",
    ")\n",
    "\n",
    "# Create an agent with the toolset \n",
    "agent = project_client.agents.create_agent(\n",
    "    model=\"gpt-4o\",  # Model deployment name\n",
    "    name=\"06-my-agent-to-be-evaluated\",  # Name of the agent\n",
    "    instructions=\"You are a helpful agent\",  # Instructions for the agent\n",
    "    toolset=toolset\n",
    ")\n",
    "print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "# Enable auto function calls for the agent\n",
    "project_client.agents.enable_auto_function_calls(toolset)\n",
    "\n",
    "# Create a thread for communication\n",
    "thread = project_client.agents.threads.create()\n",
    "print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "# Add a message to the thread\n",
    "message = project_client.agents.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",  # Role of the message sender\n",
    "    content=\"What is the weather in Bordeaux today?\",  # Message content\n",
    ")\n",
    "print(f\"Created message, ID: {message['id']}\")\n",
    "\n",
    "# Create and process an agent run\n",
    "run = project_client.agents.runs.create_and_process(thread_id=thread.id, agent_id=agent.id)\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "# Check if the run failed\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "# Fetch and log all messages\n",
    "messages = project_client.agents.messages.list(thread_id=thread.id,order=\"asc\")\n",
    "for message in messages:\n",
    "    print(f\"Role: {message['role']}\")\n",
    "    print(f\"Content: {message['content']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0300f3",
   "metadata": {},
   "source": [
    "### Get the converted data from the run/thread id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d73c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AIAgentConverter: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class FDPAgentDataRetriever: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AIAgentDataRetriever: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# Initialize the converter for Azure AI agents.\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "# Specify the thread and run ID.\n",
    "thread_id = thread.id\n",
    "run_id = run.id\n",
    "\n",
    "converted_data = converter.convert(thread_id, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442daafb",
   "metadata": {},
   "source": [
    "### Evaluate a single agent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7fa05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class IntentResolutionEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class TaskAdherenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ToolCallAccuracyEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntentResolutionEvaluator\n",
      "{\n",
      "    \"intent_resolution\": 5.0,\n",
      "    \"intent_resolution_result\": \"pass\",\n",
      "    \"intent_resolution_threshold\": 3,\n",
      "    \"intent_resolution_reason\": \"User wanted the weather in Bordeaux today. Agent provided accurate and complete information, including conditions and temperature, fully resolving the intent with clarity and relevance.\"\n",
      "}\n",
      "TaskAdherenceEvaluator\n",
      "{\n",
      "    \"task_adherence\": 5.0,\n",
      "    \"task_adherence_result\": \"pass\",\n",
      "    \"task_adherence_threshold\": 3,\n",
      "    \"task_adherence_reason\": \"The assistant correctly identified the task, used the appropriate tool, and provided a clear and accurate weather update for Bordeaux, fully satisfying the user's request.\"\n",
      "}\n",
      "ToolCallAccuracyEvaluator\n",
      "{\n",
      "    \"tool_call_accuracy\": 5.0,\n",
      "    \"tool_call_accuracy_result\": \"pass\",\n",
      "    \"tool_call_accuracy_threshold\": 3,\n",
      "    \"tool_call_accuracy_reason\": \"Let's think step by step: The user asked for the weather in Bordeaux today. The agent correctly called the 'get_weather' tool with the parameter 'city' set to 'Bordeaux', which is grounded in the user's query. The tool returned the weather information without any errors. There were no unnecessary or excessive tool calls made, and the tool call was relevant and efficient in addressing the user's query. Based on the definitions, this is an optimal solution and corresponds to a Level 5 'pass'.\",\n",
      "    \"details\": {\n",
      "        \"tool_calls_made_by_agent\": 1,\n",
      "        \"correct_tool_calls_made_by_agent\": 1,\n",
      "        \"per_tool_call_details\": [\n",
      "            {\n",
      "                \"tool_name\": \"get_weather\",\n",
      "                \"total_calls_required\": 1,\n",
      "                \"correct_calls_made_by_agent\": 1,\n",
      "                \"correct_tool_percentage\": 1.0,\n",
      "                \"tool_call_errors\": 0,\n",
      "                \"tool_success_result\": \"pass\"\n",
      "            }\n",
      "        ],\n",
      "        \"excess_tool_calls\": {\n",
      "            \"total\": 0,\n",
      "            \"details\": []\n",
      "        },\n",
      "        \"missing_tool_calls\": {\n",
      "            \"total\": 0,\n",
      "            \"details\": []\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# This is specific to agentic workflows.\n",
    "from azure.ai.evaluation import IntentResolutionEvaluator, TaskAdherenceEvaluator, ToolCallAccuracyEvaluator\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_config = {\n",
    "    \"azure_deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}\n",
    "\n",
    "# Evaluators with standard model support\n",
    "quality_evaluators = {evaluator.__name__: evaluator(model_config=model_config) for evaluator in [IntentResolutionEvaluator, TaskAdherenceEvaluator, ToolCallAccuracyEvaluator]}\n",
    "\n",
    "# Reference the quality and safety evaluator list above.\n",
    "quality_and_safety_evaluators = {**quality_evaluators}\n",
    "\n",
    "for name, evaluator in quality_and_safety_evaluators.items():\n",
    "    result = evaluator(**converted_data)\n",
    "    print(name)\n",
    "    print(json.dumps(result, indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9dde0d",
   "metadata": {},
   "source": [
    "### Evaluate multiple agent runs or threads\n",
    "\n",
    "First, convert your agent thread data into a file via our converter support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8959b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation data saved to c:\\Users\\ruplisso\\Documents\\GitHub\\genaiops-exercises2\\solutions\\06_Agents_Evaluator\\evaluation_input_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Specify a file path to save the agent output (evaluation input data) to.\n",
    "filename = os.path.join(os.getcwd(), \"evaluation_input_data.jsonl\")\n",
    "\n",
    "evaluation_data = converter.prepare_evaluation_data(thread_ids=thread_id, filename=filename) \n",
    "\n",
    "print(f\"Evaluation data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad761c9",
   "metadata": {},
   "source": [
    "Leverage the Batch evaluate API for asynchronous evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02041e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-21 15:20:52 +0200   19116 execution.bulk     INFO     Finished 1 / 7 lines.\n",
      "2025-09-21 15:20:52 +0200   19116 execution.bulk     INFO     Average execution time for completed lines: 14.83 seconds. Estimated time for incomplete lines: 88.98 seconds.\n",
      "2025-09-21 15:20:52 +0200   19116 execution.bulk     INFO     Finished 3 / 7 lines.\n",
      "2025-09-21 15:20:52 +0200   19116 execution.bulk     INFO     Average execution time for completed lines: 4.96 seconds. Estimated time for incomplete lines: 19.84 seconds.\n",
      "2025-09-21 15:20:52 +0200   19116 execution.bulk     INFO     Finished 4 / 7 lines.\n",
      "2025-09-21 15:20:52 +0200   19116 execution.bulk     INFO     Average execution time for completed lines: 3.73 seconds. Estimated time for incomplete lines: 11.19 seconds.\n",
      "2025-09-21 15:20:52 +0200   19116 execution.bulk     INFO     Finished 5 / 7 lines.\n",
      "2025-09-21 15:20:52 +0200   19116 execution.bulk     INFO     Average execution time for completed lines: 2.99 seconds. Estimated time for incomplete lines: 5.98 seconds.\n",
      "2025-09-21 15:20:52 +0200   19116 execution.bulk     INFO     Finished 6 / 7 lines.\n",
      "2025-09-21 15:20:52 +0200   19116 execution.bulk     INFO     Average execution time for completed lines: 2.5 seconds. Estimated time for incomplete lines: 2.5 seconds.\n",
      "2025-09-21 15:20:52 +0200   19116 execution.bulk     INFO     Finished 7 / 7 lines.\n",
      "2025-09-21 15:20:52 +0200   19116 execution.bulk     INFO     Average execution time for completed lines: 2.14 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"intent_resolution_20250921_132037_209943\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-09-21 13:20:37.209943+00:00\"\n",
      "Duration: \"0:00:15.429307\"\n",
      "\n",
      "2025-09-21 15:20:52 +0200   35812 execution.bulk     INFO     Finished 1 / 7 lines.\n",
      "2025-09-21 15:20:52 +0200   35812 execution.bulk     INFO     Average execution time for completed lines: 15.53 seconds. Estimated time for incomplete lines: 93.18 seconds.\n",
      "2025-09-21 15:20:52 +0200   35812 execution.bulk     INFO     Finished 2 / 7 lines.\n",
      "2025-09-21 15:20:52 +0200   35812 execution.bulk     INFO     Average execution time for completed lines: 7.77 seconds. Estimated time for incomplete lines: 38.85 seconds.\n",
      "2025-09-21 15:20:52 +0200   35812 execution.bulk     INFO     Finished 3 / 7 lines.\n",
      "2025-09-21 15:20:52 +0200   35812 execution.bulk     INFO     Average execution time for completed lines: 5.19 seconds. Estimated time for incomplete lines: 20.76 seconds.\n",
      "2025-09-21 15:20:52 +0200   35812 execution.bulk     INFO     Finished 4 / 7 lines.\n",
      "2025-09-21 15:20:52 +0200   35812 execution.bulk     INFO     Average execution time for completed lines: 3.9 seconds. Estimated time for incomplete lines: 11.7 seconds.\n",
      "2025-09-21 15:20:52 +0200   35812 execution.bulk     INFO     Finished 5 / 7 lines.\n",
      "2025-09-21 15:20:52 +0200   35812 execution.bulk     INFO     Average execution time for completed lines: 3.12 seconds. Estimated time for incomplete lines: 6.24 seconds.\n",
      "2025-09-21 15:20:52 +0200   35812 execution.bulk     INFO     Finished 6 / 7 lines.\n",
      "2025-09-21 15:20:52 +0200   35812 execution.bulk     INFO     Average execution time for completed lines: 2.6 seconds. Estimated time for incomplete lines: 2.6 seconds.\n",
      "2025-09-21 15:20:53 +0200   35812 execution.bulk     INFO     Finished 7 / 7 lines.\n",
      "2025-09-21 15:20:53 +0200   35812 execution.bulk     INFO     Average execution time for completed lines: 2.27 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-09-21 15:20:53 +0200    7364 execution.bulk     INFO     Finished 1 / 7 lines.\n",
      "2025-09-21 15:20:53 +0200    7364 execution.bulk     INFO     Average execution time for completed lines: 16.58 seconds. Estimated time for incomplete lines: 99.48 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"task_adherence_20250921_132037_232515\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-09-21 13:20:37.232515+00:00\"\n",
      "Duration: \"0:00:16.757406\"\n",
      "\n",
      "2025-09-21 15:20:54 +0200    7364 execution.bulk     INFO     Finished 2 / 7 lines.\n",
      "2025-09-21 15:20:54 +0200    7364 execution.bulk     INFO     Average execution time for completed lines: 8.67 seconds. Estimated time for incomplete lines: 43.35 seconds.\n",
      "2025-09-21 15:20:54 +0200    7364 execution.bulk     INFO     Finished 3 / 7 lines.\n",
      "2025-09-21 15:20:54 +0200    7364 execution.bulk     INFO     Average execution time for completed lines: 5.82 seconds. Estimated time for incomplete lines: 23.28 seconds.\n",
      "2025-09-21 15:20:54 +0200    7364 execution.bulk     INFO     Finished 4 / 7 lines.\n",
      "2025-09-21 15:20:54 +0200    7364 execution.bulk     INFO     Average execution time for completed lines: 4.39 seconds. Estimated time for incomplete lines: 13.17 seconds.\n",
      "2025-09-21 15:20:54 +0200    7364 execution.bulk     INFO     Finished 5 / 7 lines.\n",
      "2025-09-21 15:20:54 +0200    7364 execution.bulk     INFO     Average execution time for completed lines: 3.55 seconds. Estimated time for incomplete lines: 7.1 seconds.\n",
      "2025-09-21 15:20:54 +0200    7364 execution.bulk     INFO     Finished 6 / 7 lines.\n",
      "2025-09-21 15:20:54 +0200    7364 execution.bulk     INFO     Average execution time for completed lines: 2.96 seconds. Estimated time for incomplete lines: 2.96 seconds.\n",
      "2025-09-21 15:20:55 +0200    7364 execution.bulk     INFO     Finished 7 / 7 lines.\n",
      "2025-09-21 15:20:55 +0200    7364 execution.bulk     INFO     Average execution time for completed lines: 2.54 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"tool_call_accuracy_20250921_132037_194535\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-09-21 13:20:37.194535+00:00\"\n",
      "Duration: \"0:00:18.826168\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"tool_call_accuracy\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:18.826168\",\n",
      "        \"completed_lines\": 7,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"intent_resolution\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:15.429307\",\n",
      "        \"completed_lines\": 7,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"task_adherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:16.757406\",\n",
      "        \"completed_lines\": 7,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "{'tool_call_accuracy.tool_call_accuracy': 5.0, 'tool_call_accuracy.tool_call_accuracy_threshold': 3.0, 'intent_resolution.intent_resolution': 5.0, 'intent_resolution.intent_resolution_threshold': 3.0, 'task_adherence.task_adherence': 5.0, 'task_adherence.task_adherence_threshold': 3.0, 'tool_call_accuracy.binary_aggregate': 1.0, 'intent_resolution.binary_aggregate': 1.0, 'task_adherence.binary_aggregate': 1.0}\n",
      "AI Foundry URL: https://ai.azure.com/resource/build/evaluation/be704e19-1447-436a-ac6a-28f1f2c950ff?wsid=/subscriptions/8babb7f9-50f7-498f-9e0a-8bef4389331d/resourceGroups/rg-ruplisso-3364/providers/Microsoft.CognitiveServices/accounts/projetagent-resource/projects/projetagent&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    IntentResolutionEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "ai_project_endpoint=os.environ[\"AI_PROJECT_ENDPOINT\"]\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)\n",
    "\n",
    "response = evaluate(\n",
    "    data=filename,\n",
    "    evaluation_name=\"06- agent demo - batch run\",\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    # optionally, log your results to your Azure AI Foundry project for rich visualization \n",
    "    azure_ai_project = ai_project_endpoint\n",
    ")\n",
    "# Inspect the average scores at a high level.\n",
    "print(response[\"metrics\"])\n",
    "\n",
    "# Use the URL to inspect the results on the UI.\n",
    "print(f'AI Foundry URL: {response.get(\"studio_url\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dryrun2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
