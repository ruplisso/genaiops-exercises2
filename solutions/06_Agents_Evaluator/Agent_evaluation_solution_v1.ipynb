{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6ab610",
   "metadata": {},
   "source": [
    "https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/agent-evaluate-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33a09b",
   "metadata": {},
   "source": [
    "### Create a custom Python function and register it in the Toolset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82de94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from azure.ai.agents.models import FunctionTool, ToolSet\n",
    "from typing import Set, Callable, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "weather_api_key = os.environ.get(\"WEATHER_API_KEY\")\n",
    "\n",
    "# Define a custom Python function.\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the weather information for the specified location.\n",
    "\n",
    "    :param location (str): The location to fetch weather for.\n",
    "    :return: Weather information as a JSON string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    url = f\"http://api.weatherapi.com/v1/current.json?key={weather_api_key}&q={city}&aqi=no\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        condition = data[\"current\"][\"condition\"][\"text\"]\n",
    "        temp_c = data[\"current\"][\"temp_c\"]\n",
    "        return {\"condition\": condition,\"temperature\": temp_c}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "    \n",
    "user_functions: Set[Callable[..., Any]] = {\n",
    "    get_weather,\n",
    "}\n",
    "\n",
    "# Add tools that the agent will use. \n",
    "functions = FunctionTool(user_functions)\n",
    "\n",
    "toolset = ToolSet()\n",
    "toolset.add(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a087b31",
   "metadata": {},
   "source": [
    "### Create the agent, iniate the thread, add message and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.ai.agents \n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "# Create an Azure AI Client from an endpoint, copied from your Azure AI Foundry project.\n",
    "project_endpoint = os.environ[\"AI_PROJECT_ENDPOINT\"]  # Ensure the PROJECT_ENDPOINT environment variable is set\n",
    "\n",
    "# Create an AIProjectClient instance\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=project_endpoint,\n",
    "    credential=DefaultAzureCredential(),  # Use Azure Default Credential for authentication\n",
    ")\n",
    "\n",
    "# Create an agent with the toolset \n",
    "agent = project_client.agents.create_agent(\n",
    "    model=\"gpt-4o\",  # Model deployment name\n",
    "    name=\"06-my-agent-to-be-evaluated\",  # Name of the agent\n",
    "    instructions=\"You are a helpful agent\",  # Instructions for the agent\n",
    "    toolset=toolset\n",
    ")\n",
    "print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "# Enable auto function calls for the agent\n",
    "project_client.agents.enable_auto_function_calls(toolset)\n",
    "\n",
    "# Create a thread for communication\n",
    "thread = project_client.agents.threads.create()\n",
    "print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "# Add a message to the thread\n",
    "message = project_client.agents.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",  # Role of the message sender\n",
    "    content=\"What is the weather in Bordeaux today?\",  # Message content\n",
    ")\n",
    "print(f\"Created message, ID: {message['id']}\")\n",
    "\n",
    "# Create and process an agent run\n",
    "run = project_client.agents.runs.create_and_process(thread_id=thread.id, agent_id=agent.id)\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "# Check if the run failed\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "# Fetch and log all messages\n",
    "messages = project_client.agents.messages.list(thread_id=thread.id,order=\"asc\")\n",
    "for message in messages:\n",
    "    print(f\"Role: {message['role']}\")\n",
    "    print(f\"Content: {message['content']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0300f3",
   "metadata": {},
   "source": [
    "### Get the converted data from the run/thread id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d73c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# Initialize the converter for Azure AI agents.\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "# Specify the thread and run ID.\n",
    "thread_id = thread.id\n",
    "run_id = run.id\n",
    "\n",
    "converted_data = converter.convert(thread_id, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442daafb",
   "metadata": {},
   "source": [
    "### Evaluate a single agent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fa05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is specific to agentic workflows.\n",
    "from azure.ai.evaluation import IntentResolutionEvaluator, TaskAdherenceEvaluator, ToolCallAccuracyEvaluator\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_config = {\n",
    "    \"azure_deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}\n",
    "\n",
    "# Evaluators with standard model support\n",
    "quality_evaluators = {evaluator.__name__: evaluator(model_config=model_config) for evaluator in [IntentResolutionEvaluator, TaskAdherenceEvaluator, ToolCallAccuracyEvaluator]}\n",
    "\n",
    "# Reference the quality and safety evaluator list above.\n",
    "quality_and_safety_evaluators = {**quality_evaluators}\n",
    "\n",
    "for name, evaluator in quality_and_safety_evaluators.items():\n",
    "    result = evaluator(**converted_data)\n",
    "    print(name)\n",
    "    print(json.dumps(result, indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9dde0d",
   "metadata": {},
   "source": [
    "### Evaluate multiple agent runs or threads\n",
    "\n",
    "First, convert your agent thread data into a file via our converter support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8959b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a file path to save the agent output (evaluation input data) to.\n",
    "filename = os.path.join(os.getcwd(), \"evaluation_input_data.jsonl\")\n",
    "\n",
    "evaluation_data = converter.prepare_evaluation_data(thread_ids=thread_id, filename=filename) \n",
    "\n",
    "print(f\"Evaluation data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad761c9",
   "metadata": {},
   "source": [
    "Leverage the Batch evaluate API for asynchronous evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02041e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    IntentResolutionEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "ai_project_endpoint=os.environ[\"AI_PROJECT_ENDPOINT\"]\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)\n",
    "\n",
    "response = evaluate(\n",
    "    data=filename,\n",
    "    evaluation_name=\"06- agent demo - batch run\",\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    # optionally, log your results to your Azure AI Foundry project for rich visualization \n",
    "    azure_ai_project = ai_project_endpoint\n",
    ")\n",
    "# Inspect the average scores at a high level.\n",
    "print(response[\"metrics\"])\n",
    "\n",
    "# Use the URL to inspect the results on the UI.\n",
    "print(f'AI Foundry URL: {response.get(\"studio_url\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dryrun2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
