{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6ab610",
   "metadata": {},
   "source": [
    "https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/agent-evaluate-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33a09b",
   "metadata": {},
   "source": [
    "### Create a custom Python function and register it in the Toolset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e82de94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.agents.models import FunctionTool, ToolSet\n",
    "from typing import Set, Callable, Any\n",
    "import json\n",
    "\n",
    "# Define a custom Python function.\n",
    "def fetch_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the weather information for the specified location.\n",
    "\n",
    "    :param location (str): The location to fetch weather for.\n",
    "    :return: Weather information as a JSON string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # In a real-world scenario, you'd integrate with a weather API.\n",
    "    # In the following code snippet, we mock the response.\n",
    "    mock_weather_data = {\"Seattle\": \"Sunny, 25째C\", \"London\": \"Cloudy, 18째C\", \"Tokyo\": \"Rainy, 22째C\"}\n",
    "    weather = mock_weather_data.get(location, \"Weather data not available for this location.\")\n",
    "    weather_json = json.dumps({\"weather\": weather})\n",
    "    return weather_json\n",
    "\n",
    "user_functions: Set[Callable[..., Any]] = {\n",
    "    fetch_weather,\n",
    "}\n",
    "\n",
    "# Add tools that the agent will use. \n",
    "functions = FunctionTool(user_functions)\n",
    "\n",
    "toolset = ToolSet()\n",
    "toolset.add(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a087b31",
   "metadata": {},
   "source": [
    "### Create the agent, iniate the thread, add message and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0387b4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent, ID: asst_Dijx1EyMEpnp7SsUuOldRTKH\n",
      "Created thread, ID: thread_6cmoMfK4qazW7MJztGOrSzk8\n",
      "Created message, ID: msg_mcNTtQoSxXwug3Ju1UrebhZE\n",
      "Run finished with status: RunStatus.COMPLETED\n",
      "Role: user\n",
      "Content: [{'type': 'text', 'text': {'value': 'What is the weather in Tokyo today?', 'annotations': []}}]\n",
      "----------------------------------------\n",
      "Role: assistant\n",
      "Content: [{'type': 'text', 'text': {'value': 'The weather in Tokyo today is rainy with a temperature of 22째C.', 'annotations': []}}]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azure.ai.agents \n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "# Create an Azure AI Client from an endpoint, copied from your Azure AI Foundry project.\n",
    "project_endpoint = os.environ[\"AI_PROJECT_ENDPOINT\"]  # Ensure the PROJECT_ENDPOINT environment variable is set\n",
    "\n",
    "# Create an AIProjectClient instance\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=project_endpoint,\n",
    "    credential=DefaultAzureCredential(),  # Use Azure Default Credential for authentication\n",
    ")\n",
    "\n",
    "# Create an agent with the toolset \n",
    "agent = project_client.agents.create_agent(\n",
    "    model=\"gpt-4o\",  # Model deployment name\n",
    "    name=\"my-agent-to-be-evaluated\",  # Name of the agent\n",
    "    instructions=\"You are a helpful agent\",  # Instructions for the agent\n",
    "    toolset=toolset\n",
    ")\n",
    "print(f\"Created agent, ID: {agent.id}\")\n",
    "\n",
    "# Enable auto function calls for the agent\n",
    "project_client.agents.enable_auto_function_calls(toolset)\n",
    "\n",
    "# Create a thread for communication\n",
    "thread = project_client.agents.threads.create()\n",
    "print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "# Add a message to the thread\n",
    "message = project_client.agents.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",  # Role of the message sender\n",
    "    content=\"What is the weather in Tokyo today?\",  # Message content\n",
    ")\n",
    "print(f\"Created message, ID: {message['id']}\")\n",
    "\n",
    "# Create and process an agent run\n",
    "run = project_client.agents.runs.create_and_process(thread_id=thread.id, agent_id=agent.id)\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "# Check if the run failed\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "# Fetch and log all messages\n",
    "messages = project_client.agents.messages.list(thread_id=thread.id,order=\"asc\")\n",
    "for message in messages:\n",
    "    print(f\"Role: {message['role']}\")\n",
    "    print(f\"Content: {message['content']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0300f3",
   "metadata": {},
   "source": [
    "### Get the converted data from the run/thread id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d73c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AIAgentConverter: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class FDPAgentDataRetriever: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AIAgentDataRetriever: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# Initialize the converter for Azure AI agents.\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "# Specify the thread and run ID.\n",
    "thread_id = thread.id\n",
    "run_id = run.id\n",
    "\n",
    "converted_data = converter.convert(thread_id, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442daafb",
   "metadata": {},
   "source": [
    "### Evaluate a single agent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7fa05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class IntentResolutionEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class TaskAdherenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ToolCallAccuracyEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntentResolutionEvaluator\n",
      "{\n",
      "    \"intent_resolution\": 5.0,\n",
      "    \"intent_resolution_result\": \"pass\",\n",
      "    \"intent_resolution_threshold\": 3,\n",
      "    \"intent_resolution_reason\": \"User wanted to know the weather in Tokyo today. Agent provided an accurate and complete response, including both the condition (rainy) and temperature (22\\u00b0C), fully resolving the intent.\"\n",
      "}\n",
      "TaskAdherenceEvaluator\n",
      "{\n",
      "    \"task_adherence\": 5.0,\n",
      "    \"task_adherence_result\": \"pass\",\n",
      "    \"task_adherence_threshold\": 3,\n",
      "    \"task_adherence_reason\": \"The assistant correctly identified the task, used the appropriate tool, and provided a clear and accurate weather update for Tokyo, fully satisfying the user's request.\"\n",
      "}\n",
      "ToolCallAccuracyEvaluator\n",
      "{\n",
      "    \"tool_call_accuracy\": 5.0,\n",
      "    \"tool_call_accuracy_result\": \"pass\",\n",
      "    \"tool_call_accuracy_threshold\": 3,\n",
      "    \"tool_call_accuracy_reason\": \"Let's think step by step: The user asked for the weather in Tokyo today. The agent correctly called the 'fetch_weather' tool with the appropriate parameter 'location: Tokyo', which is grounded in the user's query. The tool returned the weather information without any errors, and the agent did not make any unnecessary or excessive tool calls. No missing tool calls were identified, and the tool call was efficient and relevant to the user's query. Based on the definitions, this is an optimal solution.\",\n",
      "    \"details\": {\n",
      "        \"tool_calls_made_by_agent\": 1,\n",
      "        \"correct_tool_calls_made_by_agent\": 1,\n",
      "        \"per_tool_call_details\": [\n",
      "            {\n",
      "                \"tool_name\": \"fetch_weather\",\n",
      "                \"total_calls_required\": 1,\n",
      "                \"correct_calls_made_by_agent\": 1,\n",
      "                \"correct_tool_percentage\": 1.0,\n",
      "                \"tool_call_errors\": 0,\n",
      "                \"tool_success_result\": \"pass\"\n",
      "            }\n",
      "        ],\n",
      "        \"excess_tool_calls\": {\n",
      "            \"total\": 0,\n",
      "            \"details\": []\n",
      "        },\n",
      "        \"missing_tool_calls\": {\n",
      "            \"total\": 0,\n",
      "            \"details\": []\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# This is specific to agentic workflows.\n",
    "from azure.ai.evaluation import IntentResolutionEvaluator, TaskAdherenceEvaluator, ToolCallAccuracyEvaluator\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_config = {\n",
    "    \"azure_deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}\n",
    "\n",
    "# Evaluators with standard model support\n",
    "quality_evaluators = {evaluator.__name__: evaluator(model_config=model_config) for evaluator in [IntentResolutionEvaluator, TaskAdherenceEvaluator, ToolCallAccuracyEvaluator]}\n",
    "\n",
    "# Reference the quality and safety evaluator list above.\n",
    "quality_and_safety_evaluators = {**quality_evaluators}\n",
    "\n",
    "for name, evaluator in quality_and_safety_evaluators.items():\n",
    "    result = evaluator(**converted_data)\n",
    "    print(name)\n",
    "    print(json.dumps(result, indent=4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9dde0d",
   "metadata": {},
   "source": [
    "### Evaluate multiple agent runs or threads\n",
    "\n",
    "First, convert your agent thread data into a file via our converter support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8959b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation data saved to c:\\Users\\ruplisso\\Documents\\GitHub\\genaiops-exercises2\\solutions\\06_Agents_Evaluator\\evaluation_input_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Specify a file path to save the agent output (evaluation input data) to.\n",
    "filename = os.path.join(os.getcwd(), \"evaluation_input_data.jsonl\")\n",
    "\n",
    "evaluation_data = converter.prepare_evaluation_data(thread_ids=thread_id, filename=filename) \n",
    "\n",
    "print(f\"Evaluation data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad761c9",
   "metadata": {},
   "source": [
    "Leverage the Batch evaluate API for asynchronous evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02041e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Finished 1 / 7 lines.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Average execution time for completed lines: 10.32 seconds. Estimated time for incomplete lines: 61.92 seconds.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Finished 2 / 7 lines.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Average execution time for completed lines: 5.17 seconds. Estimated time for incomplete lines: 25.85 seconds.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Finished 3 / 7 lines.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Average execution time for completed lines: 3.45 seconds. Estimated time for incomplete lines: 13.8 seconds.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Finished 4 / 7 lines.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Average execution time for completed lines: 2.59 seconds. Estimated time for incomplete lines: 7.77 seconds.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Finished 5 / 7 lines.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Average execution time for completed lines: 2.08 seconds. Estimated time for incomplete lines: 4.16 seconds.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Finished 6 / 7 lines.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Average execution time for completed lines: 1.77 seconds. Estimated time for incomplete lines: 1.77 seconds.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Finished 7 / 7 lines.\n",
      "2025-08-05 14:36:57 +0200    2364 execution.bulk     INFO     Average execution time for completed lines: 1.53 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"intent_resolution_20250805_123647_166519\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-08-05 12:36:47.166519+00:00\"\n",
      "Duration: \"0:00:10.962442\"\n",
      "\n",
      "2025-08-05 14:36:58 +0200   42012 execution.bulk     INFO     Finished 1 / 7 lines.\n",
      "2025-08-05 14:36:58 +0200   42012 execution.bulk     INFO     Average execution time for completed lines: 11.43 seconds. Estimated time for incomplete lines: 68.58 seconds.\n",
      "2025-08-05 14:36:58 +0200   42012 execution.bulk     INFO     Finished 3 / 7 lines.\n",
      "2025-08-05 14:36:58 +0200   42012 execution.bulk     INFO     Average execution time for completed lines: 3.82 seconds. Estimated time for incomplete lines: 15.28 seconds.\n",
      "2025-08-05 14:36:58 +0200   42012 execution.bulk     INFO     Finished 4 / 7 lines.\n",
      "2025-08-05 14:36:58 +0200   42012 execution.bulk     INFO     Average execution time for completed lines: 2.87 seconds. Estimated time for incomplete lines: 8.61 seconds.\n",
      "2025-08-05 14:36:59 +0200   42012 execution.bulk     INFO     Finished 5 / 7 lines.\n",
      "2025-08-05 14:36:59 +0200   42012 execution.bulk     INFO     Average execution time for completed lines: 2.37 seconds. Estimated time for incomplete lines: 4.74 seconds.\n",
      "2025-08-05 14:36:59 +0200   42012 execution.bulk     INFO     Finished 6 / 7 lines.\n",
      "2025-08-05 14:36:59 +0200   42012 execution.bulk     INFO     Average execution time for completed lines: 1.98 seconds. Estimated time for incomplete lines: 1.98 seconds.\n",
      "2025-08-05 14:36:59 +0200   42012 execution.bulk     INFO     Finished 7 / 7 lines.\n",
      "2025-08-05 14:36:59 +0200   42012 execution.bulk     INFO     Average execution time for completed lines: 1.7 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"task_adherence_20250805_123647_174506\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-08-05 12:36:47.174506+00:00\"\n",
      "Duration: \"0:00:11.922411\"\n",
      "\n",
      "2025-08-05 14:36:59 +0200   29824 execution.bulk     INFO     Finished 1 / 7 lines.\n",
      "2025-08-05 14:36:59 +0200   29824 execution.bulk     INFO     Average execution time for completed lines: 12.57 seconds. Estimated time for incomplete lines: 75.42 seconds.\n",
      "2025-08-05 14:36:59 +0200   29824 execution.bulk     INFO     Finished 2 / 7 lines.\n",
      "2025-08-05 14:36:59 +0200   29824 execution.bulk     INFO     Average execution time for completed lines: 6.34 seconds. Estimated time for incomplete lines: 31.7 seconds.\n",
      "2025-08-05 14:37:00 +0200   29824 execution.bulk     INFO     Finished 4 / 7 lines.\n",
      "2025-08-05 14:37:00 +0200   29824 execution.bulk     INFO     Average execution time for completed lines: 3.27 seconds. Estimated time for incomplete lines: 9.81 seconds.\n",
      "2025-08-05 14:37:00 +0200   29824 execution.bulk     INFO     Finished 5 / 7 lines.\n",
      "2025-08-05 14:37:00 +0200   29824 execution.bulk     INFO     Average execution time for completed lines: 2.62 seconds. Estimated time for incomplete lines: 5.24 seconds.\n",
      "2025-08-05 14:37:00 +0200   29824 execution.bulk     INFO     Finished 6 / 7 lines.\n",
      "2025-08-05 14:37:00 +0200   29824 execution.bulk     INFO     Average execution time for completed lines: 2.18 seconds. Estimated time for incomplete lines: 2.18 seconds.\n",
      "2025-08-05 14:37:00 +0200   29824 execution.bulk     INFO     Finished 7 / 7 lines.\n",
      "2025-08-05 14:37:00 +0200   29824 execution.bulk     INFO     Average execution time for completed lines: 1.88 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"tool_call_accuracy_20250805_123647_183602\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-08-05 12:36:47.183602+00:00\"\n",
      "Duration: \"0:00:13.774186\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"tool_call_accuracy\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:13.774186\",\n",
      "        \"completed_lines\": 7,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"intent_resolution\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:10.962442\",\n",
      "        \"completed_lines\": 7,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"task_adherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:11.922411\",\n",
      "        \"completed_lines\": 7,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "{'tool_call_accuracy.tool_call_accuracy': 5.0, 'tool_call_accuracy.tool_call_accuracy_threshold': 3.0, 'intent_resolution.intent_resolution': 5.0, 'intent_resolution.intent_resolution_threshold': 3.0, 'task_adherence.task_adherence': 5.0, 'task_adherence.task_adherence_threshold': 3.0, 'tool_call_accuracy.binary_aggregate': 1.0, 'intent_resolution.binary_aggregate': 1.0, 'task_adherence.binary_aggregate': 1.0}\n",
      "AI Foundry URL: https://ai.azure.com/resource/build/evaluation/cf08501a-01de-4f29-b130-1ef75fe3b649?wsid=/subscriptions/8babb7f9-50f7-498f-9e0a-8bef4389331d/resourceGroups/rg-ruplisso-3364/providers/Microsoft.CognitiveServices/accounts/projetagent-resource/projects/projetagent&tid=16b3c013-d300-468d-ac64-7eda0820b6d3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    IntentResolutionEvaluator,\n",
    "    TaskAdherenceEvaluator,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "ai_project_endpoint=os.environ[\"AI_PROJECT_ENDPOINT\"]\n",
    "\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)\n",
    "\n",
    "response = evaluate(\n",
    "    data=filename,\n",
    "    evaluation_name=\"agent demo - batch run\",\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    # optionally, log your results to your Azure AI Foundry project for rich visualization \n",
    "    azure_ai_project = ai_project_endpoint\n",
    ")\n",
    "# Inspect the average scores at a high level.\n",
    "print(response[\"metrics\"])\n",
    "\n",
    "# Use the URL to inspect the results on the UI.\n",
    "print(f'AI Foundry URL: {response.get(\"studio_url\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiops2envagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
