{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "This sample demonstrates how to evaluate Azure AI Agent\n",
    "Before running the sample:\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity azure-ai-evaluation\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **PROJECT_CONNECTION_STRING** - The project connection string, as found in the overview page of your Azure AI Foundry project.\n",
    "2) **MODEL_DEPLOYMENT_NAME** - The deployment name of the AI model, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n",
    "3) **AZURE_OPENAI_ENDPOINT** - Azure Open AI Endpoint to be used for evaluation.\n",
    "4) **AZURE_OPENAI_API_KEY** - Azure Open AI Key to be used for evaluation.\n",
    "5) **AZURE_OPENAI_API_VERSION** - Azure Open AI Api version to be used for evaluation.\n",
    "6) **AZURE_SUBSCRIPTION_ID** - Azure Subscription Id of Azure AI Project\n",
    "7) **PROJECT_NAME** - Azure AI Project Name\n",
    "8) **RESOURCE_GROUP_NAME** - Azure AI Project Resource Group Name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Project Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Set, Callable, Any\n",
    "import pandas as pd\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import FunctionTool, ToolSet\n",
    "# from user_functions import user_functions\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
    ")\n",
    "\n",
    "# Define a custom Python function.\n",
    "def fetch_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the weather information for the specified location.\n",
    "\n",
    "    :param location (str): The location to fetch weather for.\n",
    "    :return: Weather information as a JSON string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # In a real-world scenario, you'd integrate with a weather API.\n",
    "    # In the following code snippet, we mock the response.\n",
    "    mock_weather_data = {\"Seattle\": \"Sunny, 25°C\", \"London\": \"Cloudy, 18°C\", \"Tokyo\": \"Rainy, 22°C\"}\n",
    "    weather = mock_weather_data.get(location, \"Weather data not available for this location.\")\n",
    "    weather_json = json.dumps({\"weather\": weather})\n",
    "    return weather_json\n",
    "\n",
    "user_functions: Set[Callable[..., Any]] = {\n",
    "    fetch_weather,\n",
    "}\n",
    "\n",
    "# Adding Tools to be used by Agent \n",
    "functions = FunctionTool(user_functions)\n",
    "\n",
    "toolset = ToolSet()\n",
    "toolset.add(functions)\n",
    "\n",
    "AGENT_NAME = \"Seattle Tourist Assistant PrP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent, ID: asst_QBo4xyqwcl5v97d1Un6dyicU\n"
     ]
    }
   ],
   "source": [
    "agent = project_client.agents.create_agent(\n",
    "    model=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    "    name=AGENT_NAME,\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    toolset=toolset,\n",
    ")\n",
    "\n",
    "print(f\"Created agent, ID: {agent.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created thread, ID: thread_aQMpaAPS904c8ucbWOeBnV5W\n"
     ]
    }
   ],
   "source": [
    "thread = project_client.agents.create_thread()\n",
    "print(f\"Created thread, ID: {thread.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation with Agent\n",
    "Use below cells to have conversation with the agent\n",
    "- `Create Message[1]`\n",
    "- `Execute[2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Message[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created message, ID: msg_wslbfD0xAoj87OrHjCfS9j95\n"
     ]
    }
   ],
   "source": [
    "# Create message to thread\n",
    "\n",
    "MESSAGE = \"Can you send me an email (john@doe.com) with weather information for Seattle?\"\n",
    "\n",
    "message = project_client.agents.create_message(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=MESSAGE,\n",
    ")\n",
    "print(f\"Created message, ID: {message.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error executing function 'fetch_weather': Function 'fetch_weather' not found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run finished with status: RunStatus.COMPLETED\n",
      "Run ID: run_AoKtc3888s9a7g10r08odqav\n"
     ]
    }
   ],
   "source": [
    "run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "print(f\"Run ID: {run.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: MessageRole.USER\n",
      "Content: Can you send me an email (john@doe.com) with weather information for Seattle?\n",
      "----------------------------------------\n",
      "Role: MessageRole.AGENT\n",
      "Content: It seems I'm unable to retrieve the weather information right now. However, I can assist you in other ways or suggest alternatives for checking Seattle's weather—such as using a reliable weather website like Weather.com or AccuWeather.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for message in project_client.agents.list_messages(thread.id, order=\"asc\").data:\n",
    "    print(f\"Role: {message.role}\")\n",
    "    print(f\"Content: {message.content[0].text.value}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# Initialize the converter that will be backed by the project.\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "thread_id = thread.id\n",
    "run_id = run.id\n",
    "\n",
    "converted_data = converter.convert(thread_id, run_id)\n",
    "print(json.dumps(converted_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the converted data to a JSONL file\n",
    "\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "evaluation_data = converter.prepare_evaluation_data(thread_ids=thread.id, filename=file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import ToolCallAccuracyEvaluator , AzureOpenAIModelConfiguration, IntentResolutionEvaluator, TaskAdherenceEvaluator, ViolenceEvaluator\n",
    "from pprint import pprint\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    ")\n",
    "# Needed to use content safety evaluators\n",
    "azure_ai_project={\n",
    "    \"subscription_id\": os.environ[\"SUBSCRIPTION_ID\"],\n",
    "    \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "    \"resource_group_name\": os.environ[\"RG_NAME\"],\n",
    "}\n",
    "\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent_resolution': 5.0,\n",
       " 'intent_resolution_result': 'pass',\n",
       " 'intent_resolution_threshold': 3,\n",
       " 'intent_resolution_reason': \"The response provides the opening hours of the Eiffel Tower, which directly addresses the user's query with accurate and complete information. No additional details or tools were required to resolve the query.\",\n",
       " 'additional_details': {'conversation_has_intent': True,\n",
       "  'agent_perceived_intent': 'provide the opening hours of the Eiffel Tower',\n",
       "  'actual_user_intent': 'find out the opening hours of the Eiffel Tower',\n",
       "  'correct_intent_detected': True,\n",
       "  'intent_resolved': True}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating query and response as strings\n",
    "# A positive example. Intent is identified and understood and the response correctly resolves user intent\n",
    "result = intent_resolution(\n",
    "    query=\"What are the opening hours of the Eiffel Tower?\",\n",
    "    response=\"Opening hours of the Eiffel Tower are 9:00 AM to 11:00 PM.\",\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_completeness': 2,\n",
       " 'response_completeness_result': 'fail',\n",
       " 'response_completeness_threshold': 3,\n",
       " 'response_completeness_reason': 'The response is barely complete because it only matches the Day 2 activity and completely diverges from the ground truth for Day 1.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import ResponseCompletenessEvaluator\n",
    "\n",
    "response_completeness = ResponseCompletenessEvaluator(model_config=model_config)\n",
    "# A negative example. Only half of the statements in the response were complete according to the ground truth  \n",
    "result = response_completeness(\n",
    "    response=\"Itinery: Day 1 take a train to visit Disneyland outside of the city; Day 2 rests in hotel.\",\n",
    "    ground_truth=\"Itinery: Day 1 take a train to visit the downtown area for city sightseeing; Day 2 rests in hotel.\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call_accuracy': 0.5,\n",
       " 'tool_call_accuracy_result': 'fail',\n",
       " 'tool_call_accuracy_threshold': 0.8,\n",
       " 'per_tool_call_details': [{'tool_call_accurate': True,\n",
       "   'tool_call_accurate_reason': \"The TOOL CALL is relevant, uses appropriate parameters, and extracts correct values from the conversation, making it highly likely to resolve the user's need.\",\n",
       "   'tool_call_id': 'call_CUdbkBfvVBla2YP3p24uhElJ'},\n",
       "  {'tool_call_accurate': False,\n",
       "   'tool_call_accurate_reason': 'The TOOL CALL is not relevant because the parameter value (\"London\") does not match the user\\'s query about \"Seattle,\" and thus it will not help resolve the user\\'s need.',\n",
       "   'tool_call_id': 'call_CUdbkBfvVBla2YP3p24uhElJ'}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How is the weather in Seattle?\"\n",
    "tool_calls = [{\n",
    "                    \"type\": \"tool_call\",\n",
    "                    \"tool_call_id\": \"call_CUdbkBfvVBla2YP3p24uhElJ\",\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"arguments\": {\n",
    "                        \"location\": \"Seattle\"\n",
    "                    }\n",
    "            },\n",
    "            {\n",
    "                    \"type\": \"tool_call\",\n",
    "                    \"tool_call_id\": \"call_CUdbkBfvVBla2YP3p24uhElJ\",\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"arguments\": {\n",
    "                        \"location\": \"London\"\n",
    "                    }\n",
    "            }\n",
    "            ]\n",
    "\n",
    "tool_definition = {\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"description\": \"Fetches the weather information for the specified location.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The location to fetch weather for.\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "response = tool_call_accuracy(query=query, tool_calls=tool_calls, tool_definitions=tool_definition)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-28 11:07:18 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 11:07:18 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 11:07:18 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 11:07:18 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kdpkq223_20250728_110718_082694, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kdpkq223_20250728_110718_082694\\logs.txt\n",
      "[2025-07-28 11:07:18 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_4afnhe7v_20250728_110718_100631, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_4afnhe7v_20250728_110718_100631\\logs.txt\n",
      "[2025-07-28 11:07:18 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_yhwzlm_5_20250728_110718_100631, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_yhwzlm_5_20250728_110718_100631\\logs.txt\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# Regenerate the evaluation data file to ensure it is valid\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "evaluation_data = converter.prepare_evaluation_data(thread_ids=thread.id, filename=file_name)\n",
    "\n",
    "response = evaluate(\n",
    "    data=file_name,\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"RG_NAME\"],\n",
    "    }\n",
    ")\n",
    "pprint(f'AI Foundary URL: {response.get(\"studio_url\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-28 11:07:52 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 11:07:52 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 11:07:52 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 11:07:52 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 11:07:52 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_zc8uiq8z_20250728_110751_969460, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_zc8uiq8z_20250728_110751_969460\\logs.txt\n",
      "[2025-07-28 11:07:52 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_b9o5c8_s_20250728_110751_961797, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_b9o5c8_s_20250728_110751_961797\\logs.txt\n",
      "[2025-07-28 11:07:52 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_bfu_giht_20250728_110751_956640, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_bfu_giht_20250728_110751_956640\\logs.txt\n",
      "[2025-07-28 11:07:52 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kbay17af_20250728_110751_971474, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kbay17af_20250728_110751_971474\\logs.txt\n",
      "[2025-07-28 11:07:56 +0200][promptflow._sdk._orchestrator.run_submitter][WARNING] - 90 out of 90 runs failed in batch run.\n",
      " Please check out C:/Users/ruplisso/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_zc8uiq8z_20250728_110751_969460 for more details.\n",
      "[2025-07-28 11:07:56 +0200][promptflow._sdk._orchestrator.run_submitter][WARNING] - 90 out of 90 runs failed in batch run.\n",
      " Please check out C:/Users/ruplisso/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kbay17af_20250728_110751_971474 for more details.\n",
      "[2025-07-28 11:07:56 +0200][promptflow._sdk._orchestrator.run_submitter][WARNING] - 90 out of 90 runs failed in batch run.\n",
      " Please check out C:/Users/ruplisso/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_bfu_giht_20250728_110751_956640 for more details.\n",
      "[2025-07-28 11:07:56 +0200][promptflow._sdk._orchestrator.run_submitter][WARNING] - 90 out of 90 runs failed in batch run.\n",
      " Please check out C:/Users/ruplisso/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_b9o5c8_s_20250728_110751_961797 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://ai.azure.com/build/evaluation/42ec0a3e-9891-400f-b169-135e9327ecb4?wsid=/subscriptions/8babb7f9-50f7-498f-9e0a-8bef4389331d/resourceGroups/rg-ProjectFoundryHub03072025_2/providers/Microsoft.MachineLearningServices/workspaces/ProjectFoundryHub03072025_2'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = evaluate(\n",
    "    data=\"./sample_synthetic_conversations.jsonl\",\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "        \"response_completeness\": response_completeness,\n",
    "    },\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"RG_NAME\"],\n",
    "    },\n",
    ")\n",
    "response.get(\"studio_url\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiops2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
