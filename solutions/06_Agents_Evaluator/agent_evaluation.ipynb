{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "This sample demonstrates how to evaluate Azure AI Agent\n",
    "Before running the sample:\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity azure-ai-evaluation\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **PROJECT_CONNECTION_STRING** - The project connection string, as found in the overview page of your Azure AI Foundry project.\n",
    "2) **MODEL_DEPLOYMENT_NAME** - The deployment name of the AI model, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n",
    "3) **AZURE_OPENAI_ENDPOINT** - Azure Open AI Endpoint to be used for evaluation.\n",
    "4) **AZURE_OPENAI_API_KEY** - Azure Open AI Key to be used for evaluation.\n",
    "5) **AZURE_OPENAI_API_VERSION** - Azure Open AI Api version to be used for evaluation.\n",
    "6) **AZURE_SUBSCRIPTION_ID** - Azure Subscription Id of Azure AI Project\n",
    "7) **PROJECT_NAME** - Azure AI Project Name\n",
    "8) **RESOURCE_GROUP_NAME** - Azure AI Project Resource Group Name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Project Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Set, Callable, Any\n",
    "import pandas as pd\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import FunctionTool, ToolSet\n",
    "# from user_functions import user_functions\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
    ")\n",
    "\n",
    "# Define a custom Python function.\n",
    "def fetch_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the weather information for the specified location.\n",
    "\n",
    "    :param location (str): The location to fetch weather for.\n",
    "    :return: Weather information as a JSON string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # In a real-world scenario, you'd integrate with a weather API.\n",
    "    # In the following code snippet, we mock the response.\n",
    "    mock_weather_data = {\"Seattle\": \"Sunny, 25°C\", \"London\": \"Cloudy, 18°C\", \"Tokyo\": \"Rainy, 22°C\"}\n",
    "    weather = mock_weather_data.get(location, \"Weather data not available for this location.\")\n",
    "    weather_json = json.dumps({\"weather\": weather})\n",
    "    return weather_json\n",
    "\n",
    "user_functions: Set[Callable[..., Any]] = {\n",
    "    fetch_weather,\n",
    "}\n",
    "\n",
    "# Adding Tools to be used by Agent \n",
    "functions = FunctionTool(user_functions)\n",
    "\n",
    "toolset = ToolSet()\n",
    "toolset.add(functions)\n",
    "\n",
    "AGENT_NAME = \"Seattle Tourist Assistant PrP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent, ID: asst_QBo4xyqwcl5v97d1Un6dyicU\n"
     ]
    }
   ],
   "source": [
    "agent = project_client.agents.create_agent(\n",
    "    model=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    "    name=AGENT_NAME,\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    toolset=toolset,\n",
    ")\n",
    "\n",
    "print(f\"Created agent, ID: {agent.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created thread, ID: thread_aQMpaAPS904c8ucbWOeBnV5W\n"
     ]
    }
   ],
   "source": [
    "thread = project_client.agents.create_thread()\n",
    "print(f\"Created thread, ID: {thread.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation with Agent\n",
    "Use below cells to have conversation with the agent\n",
    "- `Create Message[1]`\n",
    "- `Execute[2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Message[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created message, ID: msg_wslbfD0xAoj87OrHjCfS9j95\n"
     ]
    }
   ],
   "source": [
    "# Create message to thread\n",
    "\n",
    "MESSAGE = \"Can you send me an email (john@doe.com) with weather information for Seattle?\"\n",
    "\n",
    "message = project_client.agents.create_message(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=MESSAGE,\n",
    ")\n",
    "print(f\"Created message, ID: {message.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error executing function 'fetch_weather': Function 'fetch_weather' not found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run finished with status: RunStatus.COMPLETED\n",
      "Run ID: run_AoKtc3888s9a7g10r08odqav\n"
     ]
    }
   ],
   "source": [
    "run = project_client.agents.create_and_process_run(thread_id=thread.id, agent_id=agent.id)\n",
    "\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "print(f\"Run ID: {run.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: MessageRole.USER\n",
      "Content: Can you send me an email (john@doe.com) with weather information for Seattle?\n",
      "----------------------------------------\n",
      "Role: MessageRole.AGENT\n",
      "Content: It seems I'm unable to retrieve the weather information right now. However, I can assist you in other ways or suggest alternatives for checking Seattle's weather—such as using a reliable weather website like Weather.com or AccuWeather.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for message in project_client.agents.list_messages(thread.id, order=\"asc\").data:\n",
    "    print(f\"Role: {message.role}\")\n",
    "    print(f\"Content: {message.content[0].text.value}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Could not import RedTeam. Please install the dependency with `pip install azure-ai-evaluation[redteam]`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:azure.ai.evaluation._common._experimental:Class AIAgentConverter: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"query\": [\n",
      "        {\n",
      "            \"role\": \"system\",\n",
      "            \"content\": \"You are a helpful assistant\"\n",
      "        },\n",
      "        {\n",
      "            \"createdAt\": \"2025-07-28T08:16:21Z\",\n",
      "            \"role\": \"user\",\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"type\": \"text\",\n",
      "                    \"text\": \"Can you send me an email (john@doe.com) with weather information for Seattle?\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"response\": [\n",
      "        {\n",
      "            \"createdAt\": \"2025-07-28T08:16:28Z\",\n",
      "            \"run_id\": \"run_AoKtc3888s9a7g10r08odqav\",\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"type\": \"tool_call\",\n",
      "                    \"tool_call_id\": \"call_b34XA2a7eSN6NQComJGq3HGG\",\n",
      "                    \"name\": \"fetch_weather\",\n",
      "                    \"arguments\": {\n",
      "                        \"location\": \"Seattle\"\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"createdAt\": \"2025-07-28T08:16:29Z\",\n",
      "            \"run_id\": \"run_AoKtc3888s9a7g10r08odqav\",\n",
      "            \"tool_call_id\": \"call_b34XA2a7eSN6NQComJGq3HGG\",\n",
      "            \"role\": \"tool\",\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"type\": \"tool_result\",\n",
      "                    \"tool_result\": {\n",
      "                        \"error\": \"Error executing function 'fetch_weather': Function 'fetch_weather' not found.\"\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"createdAt\": \"2025-07-28T08:16:30Z\",\n",
      "            \"run_id\": \"run_AoKtc3888s9a7g10r08odqav\",\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"type\": \"text\",\n",
      "                    \"text\": \"It seems I'm unable to retrieve the weather information right now. However, I can assist you in other ways or suggest alternatives for checking Seattle's weather\\u2014such as using a reliable weather website like Weather.com or AccuWeather.\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"tool_definitions\": [\n",
      "        {\n",
      "            \"name\": \"fetch_weather\",\n",
      "            \"description\": \"Fetches the weather information for the specified location.\",\n",
      "            \"parameters\": {\n",
      "                \"type\": \"object\",\n",
      "                \"properties\": {\n",
      "                    \"location\": {\n",
      "                        \"type\": \"string\",\n",
      "                        \"description\": \"The location to fetch weather for.\"\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from azure.ai.evaluation import AIAgentConverter\n",
    "\n",
    "# Initialize the converter that will be backed by the project.\n",
    "converter = AIAgentConverter(project_client)\n",
    "\n",
    "thread_id = thread.id\n",
    "run_id = run.id\n",
    "\n",
    "converted_data = converter.convert(thread_id, run_id)\n",
    "print(json.dumps(converted_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the converted data to a JSONL file\n",
    "\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "evaluation_data = converter.prepare_evaluation_data(thread_ids=thread.id, filename=file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:azure.ai.evaluation._common._experimental:Class ToolCallAccuracyEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "WARNING:azure.ai.evaluation._common._experimental:Class IntentResolutionEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "WARNING:azure.ai.evaluation._common._experimental:Class TaskAdherenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import ToolCallAccuracyEvaluator , AzureOpenAIModelConfiguration, IntentResolutionEvaluator, TaskAdherenceEvaluator, ViolenceEvaluator\n",
    "from pprint import pprint\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    ")\n",
    "# Needed to use content safety evaluators\n",
    "azure_ai_project={\n",
    "    \"subscription_id\": os.environ[\"SUBSCRIPTION_ID\"],\n",
    "    \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "    \"resource_group_name\": os.environ[\"RG_NAME\"],\n",
    "}\n",
    "\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(model_config=model_config)\n",
    "intent_resolution = IntentResolutionEvaluator(model_config=model_config)\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent_resolution': 5.0,\n",
       " 'intent_resolution_result': 'pass',\n",
       " 'intent_resolution_threshold': 3,\n",
       " 'intent_resolution_reason': \"The response provides the opening hours of the Eiffel Tower, which directly addresses the user's query with accurate and complete information. No additional details or tools were required to resolve the query.\",\n",
       " 'additional_details': {'conversation_has_intent': True,\n",
       "  'agent_perceived_intent': 'provide the opening hours of the Eiffel Tower',\n",
       "  'actual_user_intent': 'find out the opening hours of the Eiffel Tower',\n",
       "  'correct_intent_detected': True,\n",
       "  'intent_resolved': True}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating query and response as strings\n",
    "# A positive example. Intent is identified and understood and the response correctly resolves user intent\n",
    "result = intent_resolution(\n",
    "    query=\"What are the opening hours of the Eiffel Tower?\",\n",
    "    response=\"Opening hours of the Eiffel Tower are 9:00 AM to 11:00 PM.\",\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_completeness': 2,\n",
       " 'response_completeness_result': 'fail',\n",
       " 'response_completeness_threshold': 3,\n",
       " 'response_completeness_reason': 'The response is barely complete because it only matches one part of the ground truth (Day 2) and misses the key information for Day 1.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import ResponseCompletenessEvaluator\n",
    "\n",
    "response_completeness = ResponseCompletenessEvaluator(model_config=model_config)\n",
    "# A negative example. Only half of the statements in the response were complete according to the ground truth  \n",
    "result = response_completeness(\n",
    "    response=\"Itinery: Day 1 take a train to visit Disneyland outside of the city; Day 2 rests in hotel.\",\n",
    "    ground_truth=\"Itinery: Day 1 take a train to visit the downtown area for city sightseeing; Day 2 rests in hotel.\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call_accuracy': 0.5,\n",
       " 'tool_call_accuracy_result': 'fail',\n",
       " 'tool_call_accuracy_threshold': 0.8,\n",
       " 'per_tool_call_details': [{'tool_call_accurate': True,\n",
       "   'tool_call_accurate_reason': \"The TOOL CALL is relevant, uses appropriate parameters, and extracts correct values from the conversation, making it highly likely to resolve the user's need.\",\n",
       "   'tool_call_id': 'call_CUdbkBfvVBla2YP3p24uhElJ'},\n",
       "  {'tool_call_accurate': False,\n",
       "   'tool_call_accurate_reason': 'The TOOL CALL is irrelevant because the parameter value (\"London\") does not align with the user\\'s query about Seattle, and thus it fails to address the user\\'s need. The tool definition is followed, but the parameter value correctness is not met.',\n",
       "   'tool_call_id': 'call_CUdbkBfvVBla2YP3p24uhElJ'}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How is the weather in Seattle?\"\n",
    "tool_calls = [{\n",
    "                    \"type\": \"tool_call\",\n",
    "                    \"tool_call_id\": \"call_CUdbkBfvVBla2YP3p24uhElJ\",\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"arguments\": {\n",
    "                        \"location\": \"Seattle\"\n",
    "                    }\n",
    "            },\n",
    "            {\n",
    "                    \"type\": \"tool_call\",\n",
    "                    \"tool_call_id\": \"call_CUdbkBfvVBla2YP3p24uhElJ\",\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"arguments\": {\n",
    "                        \"location\": \"London\"\n",
    "                    }\n",
    "            }\n",
    "            ]\n",
    "\n",
    "tool_definition = {\n",
    "                    \"name\": \"fetch_weather\",\n",
    "                    \"description\": \"Fetches the weather information for the specified location.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The location to fetch weather for.\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "response = tool_call_accuracy(query=query, tool_calls=tool_calls, tool_definitions=tool_definition)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "EvaluationException",
     "evalue": "(UserError) Unable to load data from 'evaluation_data.jsonl'. Supported formats are JSONL and CSV. Detailed error: Expected object or value.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruplisso\\AppData\\Local\\miniconda3\\envs\\genaiops2env\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluate\\_evaluate.py:466\u001b[39m, in \u001b[36m_validate_and_load_data\u001b[39m\u001b[34m(target, data, evaluators, output_path, azure_ai_project, evaluation_name)\u001b[39m\n\u001b[32m    465\u001b[39m     data_loader = DataLoaderFactory.get_loader(data)\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     initial_data_df = \u001b[43mdata_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruplisso\\AppData\\Local\\miniconda3\\envs\\genaiops2env\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluate\\_utils.py:338\u001b[39m, in \u001b[36mJSONLDataFileLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> pd.DataFrame:\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruplisso\\AppData\\Local\\miniconda3\\envs\\genaiops2env\\Lib\\site-packages\\pandas\\io\\json\\_json.py:815\u001b[39m, in \u001b[36mread_json\u001b[39m\u001b[34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[39m\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruplisso\\AppData\\Local\\miniconda3\\envs\\genaiops2env\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1023\u001b[39m, in \u001b[36mJsonReader.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1022\u001b[39m         data_lines = data.split(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m         obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruplisso\\AppData\\Local\\miniconda3\\envs\\genaiops2env\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1051\u001b[39m, in \u001b[36mJsonReader._get_object_parser\u001b[39m\u001b[34m(self, json)\u001b[39m\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mframe\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m     obj = \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33mseries\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruplisso\\AppData\\Local\\miniconda3\\envs\\genaiops2env\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1187\u001b[39m, in \u001b[36mParser.parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1185\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1186\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1187\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1189\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruplisso\\AppData\\Local\\miniconda3\\envs\\genaiops2env\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1403\u001b[39m, in \u001b[36mFrameParser._parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m orient == \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1402\u001b[39m     \u001b[38;5;28mself\u001b[39m.obj = DataFrame(\n\u001b[32m-> \u001b[39m\u001b[32m1403\u001b[39m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1404\u001b[39m     )\n\u001b[32m   1405\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m orient == \u001b[33m\"\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: Expected object or value",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mEvaluationException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# file_name = \"agent_evaluation_dataset_one_line.jsonl\"\u001b[39;00m\n\u001b[32m      4\u001b[39m file_name = \u001b[33m\"\u001b[39m\u001b[33mevaluation_data.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_call_accuracy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_accuracy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mintent_resolution\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mintent_resolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtask_adherence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_adherence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msubscription_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSUBSCRIPTION_ID\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproject_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPROJECT_NAME\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresource_group_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRG_NAME\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m pprint(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAI Foundary URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.get(\u001b[33m\"\u001b[39m\u001b[33mstudio_url\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruplisso\\AppData\\Local\\miniconda3\\envs\\genaiops2env\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluate\\_evaluate.py:705\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(data, evaluators, evaluation_name, target, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, **kwargs)\u001b[39m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EvaluationException):\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[32m    699\u001b[39m         message=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    700\u001b[39m         target=ErrorTarget.EVALUATE,\n\u001b[32m    701\u001b[39m         category=ErrorCategory.FAILED_EXECUTION,\n\u001b[32m    702\u001b[39m         blame=ErrorBlame.SYSTEM_ERROR,\n\u001b[32m    703\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruplisso\\AppData\\Local\\miniconda3\\envs\\genaiops2env\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluate\\_evaluate.py:663\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(data, evaluators, evaluation_name, target, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Evaluates target or data with built-in or custom evaluators. If both target and data are provided,\u001b[39;00m\n\u001b[32m    623\u001b[39m \u001b[33;03m    data will be run through target function and then results will be evaluated.\u001b[39;00m\n\u001b[32m    624\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    660\u001b[39m \u001b[33;03m        :caption: Run an evaluation on local data with Coherence and Relevance evaluators.\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m=\u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfail_on_evaluator_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfail_on_evaluator_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# Handle multiprocess bootstrap error\u001b[39;00m\n\u001b[32m    676\u001b[39m     bootstrap_error = (\n\u001b[32m    677\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn attempt has been made to start a new process before the\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m        \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    678\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcurrent process has finished its bootstrapping phase.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    679\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruplisso\\AppData\\Local\\miniconda3\\envs\\genaiops2env\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluate\\_evaluate.py:744\u001b[39m, in \u001b[36m_evaluate\u001b[39m\u001b[34m(evaluators, evaluation_name, target, data, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, **kwargs)\u001b[39m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fail_on_evaluator_errors:\n\u001b[32m    743\u001b[39m     _print_fail_flag_warning()\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m input_data_df = \u001b[43m_validate_and_load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;66;03m# Process evaluator config to replace ${target.} with ${data.}\u001b[39;00m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m evaluator_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ruplisso\\AppData\\Local\\miniconda3\\envs\\genaiops2env\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluate\\_evaluate.py:468\u001b[39m, in \u001b[36m_validate_and_load_data\u001b[39m\u001b[34m(target, data, evaluators, output_path, azure_ai_project, evaluation_name)\u001b[39m\n\u001b[32m    466\u001b[39m     initial_data_df = data_loader.load()\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[32m    469\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to load data from \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Supported formats are JSONL and CSV. Detailed error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    470\u001b[39m         target=ErrorTarget.EVALUATE,\n\u001b[32m    471\u001b[39m         category=ErrorCategory.INVALID_VALUE,\n\u001b[32m    472\u001b[39m         blame=ErrorBlame.USER_ERROR,\n\u001b[32m    473\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m initial_data_df\n",
      "\u001b[31mEvaluationException\u001b[39m: (UserError) Unable to load data from 'evaluation_data.jsonl'. Supported formats are JSONL and CSV. Detailed error: Expected object or value."
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# Regenerate the evaluation data file to ensure it is valid\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "evaluation_data = converter.prepare_evaluation_data(thread_ids=thread.id, filename=file_name)\n",
    "\n",
    "response = evaluate(\n",
    "    data=file_name,\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "    },\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"RG_NAME\"],\n",
    "    }\n",
    ")\n",
    "pprint(f'AI Foundary URL: {response.get(\"studio_url\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-28 10:54:05 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 10:54:05 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 10:54:05 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 10:54:05 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 10:54:05 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ih6v07_8_20250728_105404_742609, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ih6v07_8_20250728_105404_742609\\logs.txt\n",
      "[2025-07-28 10:54:05 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_pd_hx2n7_20250728_105404_742609, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_pd_hx2n7_20250728_105404_742609\\logs.txt\n",
      "[2025-07-28 10:54:05 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_vmgve3nu_20250728_105404_742609, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_vmgve3nu_20250728_105404_742609\\logs.txt\n",
      "[2025-07-28 10:54:05 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_k5dui4q8_20250728_105404_740260, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_k5dui4q8_20250728_105404_740260\\logs.txt\n",
      "[2025-07-28 10:54:09 +0200][promptflow._sdk._orchestrator.run_submitter][WARNING] - 90 out of 90 runs failed in batch run.\n",
      " Please check out C:/Users/ruplisso/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_k5dui4q8_20250728_105404_740260 for more details.\n",
      "[2025-07-28 10:54:09 +0200][promptflow._sdk._orchestrator.run_submitter][WARNING] - 90 out of 90 runs failed in batch run.\n",
      " Please check out C:/Users/ruplisso/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_vmgve3nu_20250728_105404_742609 for more details.\n",
      "[2025-07-28 10:54:09 +0200][promptflow._sdk._orchestrator.run_submitter][WARNING] - 90 out of 90 runs failed in batch run.\n",
      " Please check out C:/Users/ruplisso/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ih6v07_8_20250728_105404_742609 for more details.\n",
      "[2025-07-28 10:54:09 +0200][promptflow._sdk._orchestrator.run_submitter][WARNING] - 90 out of 90 runs failed in batch run.\n",
      " Please check out C:/Users/ruplisso/.promptflow/.runs/azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_pd_hx2n7_20250728_105404_742609 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 10:54:05 +0200   18668 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-07-28 10:54:09 +0200   18668 execution.bulk     INFO     Finished 90 / 90 lines.\n",
      "2025-07-28 10:54:09 +0200   18668 execution.bulk     INFO     Average execution time for completed lines: 0.04 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-07-28 10:54:09 +0200   18668 execution          ERROR    90/90 flow run failed, indexes: [5,59,6,60,7,61,8,62,63,9,10,64,11,65,12,66,13,67,14,68,26,15,69,27,16,70,28,25,71,29,17,72,30,18,73,31,19,74,32,20,75,33,21,76,34,22,77,35,23,78,36,79,24,37,80,38,81,39,82,40,83,0,41,84,42,85,43,86,44,87,58,45,88,46,89,47,48,1,49,50,3,51,52,2,53,54,55,56,57,4], exception of index 5: (UserError) TaskAdherenceEvaluator: Either 'conversation' or individual inputs must be provided.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_k5dui4q8_20250728_105404_740260\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-07-28 10:54:04.724808+02:00\"\n",
      "Duration: \"0:00:08.559040\"\n",
      "Output path: \"C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_k5dui4q8_20250728_105404_740260\"\n",
      "\n",
      "2025-07-28 10:54:05 +0200   18668 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-07-28 10:54:09 +0200   18668 execution.bulk     INFO     Finished 90 / 90 lines.\n",
      "2025-07-28 10:54:09 +0200   18668 execution.bulk     INFO     Average execution time for completed lines: 0.04 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-07-28 10:54:09 +0200   18668 execution          ERROR    90/90 flow run failed, indexes: [47,23,48,24,49,50,51,22,52,53,54,55,56,57,58,0,59,60,1,61,62,3,63,64,2,65,66,67,68,26,69,27,4,70,28,5,71,29,6,72,30,7,73,31,8,74,32,9,75,33,10,76,34,11,77,35,78,12,36,13,79,37,14,80,38,15,81,39,16,82,40,25,83,41,17,84,42,18,85,43,19,86,44,20,87,45,21,88,46,89], exception of index 47: (UserError) ResponseCompletenessEvaluator: Either 'conversation' or individual inputs must be provided.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ih6v07_8_20250728_105404_742609\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-07-28 10:54:04.724808+02:00\"\n",
      "Duration: \"0:00:08.579404\"\n",
      "Output path: \"C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ih6v07_8_20250728_105404_742609\"\n",
      "\n",
      "2025-07-28 10:54:05 +0200   18668 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-07-28 10:54:09 +0200   18668 execution.bulk     INFO     Finished 90 / 90 lines.\n",
      "2025-07-28 10:54:09 +0200   18668 execution.bulk     INFO     Average execution time for completed lines: 0.04 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-07-28 10:54:09 +0200   18668 execution          ERROR    90/90 flow run failed, indexes: [44,87,45,88,46,89,0,47,48,1,49,50,3,51,52,2,53,54,55,56,57,4,58,5,59,6,60,7,61,8,62,9,63,10,64,11,65,12,66,13,67,14,68,26,15,69,27,16,70,28,25,71,29,17,72,30,18,73,31,19,74,32,20,75,33,21,76,34,22,77,35,23,78,36,24,79,37,80,38,81,39,82,40,83,41,84,42,85,43,86], exception of index 44: (UserError) Either response or tool_calls must be provided.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_pd_hx2n7_20250728_105404_742609\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-07-28 10:54:04.724808+02:00\"\n",
      "Duration: \"0:00:08.598820\"\n",
      "Output path: \"C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_pd_hx2n7_20250728_105404_742609\"\n",
      "\n",
      "2025-07-28 10:54:05 +0200   18668 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-07-28 10:54:09 +0200   18668 execution.bulk     INFO     Finished 90 / 90 lines.\n",
      "2025-07-28 10:54:09 +0200   18668 execution.bulk     INFO     Average execution time for completed lines: 0.04 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-07-28 10:54:09 +0200   18668 execution          ERROR    90/90 flow run failed, indexes: [18,73,31,19,74,32,75,20,33,21,76,34,22,77,35,78,36,79,37,80,38,81,39,82,40,83,41,84,42,85,43,86,44,87,45,88,46,89,47,48,1,49,50,0,51,52,53,54,55,2,56,3,57,4,58,5,59,6,60,7,61,62,8,9,63,10,64,11,65,12,66,23,13,67,24,14,68,26,15,69,27,16,70,28,25,71,29,17,72,30], exception of index 18: (UserError) IntentResolutionEvaluator: Either 'conversation' or individual inputs must be provided.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_vmgve3nu_20250728_105404_742609\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-07-28 10:54:04.724808+02:00\"\n",
      "Duration: \"0:00:08.559040\"\n",
      "Output path: \"C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_vmgve3nu_20250728_105404_742609\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"tool_call_accuracy\": {\n",
      "        \"status\": \"Completed with Errors\",\n",
      "        \"duration\": \"0:00:08.598820\",\n",
      "        \"completed_lines\": 0,\n",
      "        \"failed_lines\": 90,\n",
      "        \"log_path\": \"C:\\\\Users\\\\ruplisso\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_pd_hx2n7_20250728_105404_742609\"\n",
      "    },\n",
      "    \"intent_resolution\": {\n",
      "        \"status\": \"Completed with Errors\",\n",
      "        \"duration\": \"0:00:08.559040\",\n",
      "        \"completed_lines\": 0,\n",
      "        \"failed_lines\": 90,\n",
      "        \"log_path\": \"C:\\\\Users\\\\ruplisso\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_vmgve3nu_20250728_105404_742609\"\n",
      "    },\n",
      "    \"task_adherence\": {\n",
      "        \"status\": \"Completed with Errors\",\n",
      "        \"duration\": \"0:00:08.559040\",\n",
      "        \"completed_lines\": 0,\n",
      "        \"failed_lines\": 90,\n",
      "        \"log_path\": \"C:\\\\Users\\\\ruplisso\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_k5dui4q8_20250728_105404_740260\"\n",
      "    },\n",
      "    \"response_completeness\": {\n",
      "        \"status\": \"Completed with Errors\",\n",
      "        \"duration\": \"0:00:08.579404\",\n",
      "        \"completed_lines\": 0,\n",
      "        \"failed_lines\": 90,\n",
      "        \"log_path\": \"C:\\\\Users\\\\ruplisso\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_ih6v07_8_20250728_105404_742609\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://ai.azure.com/build/evaluation/83d08250-70b2-43f0-85e6-4ff837dc527b?wsid=/subscriptions/8babb7f9-50f7-498f-9e0a-8bef4389331d/resourceGroups/rg-ProjectFoundryHub03072025_2/providers/Microsoft.MachineLearningServices/workspaces/ProjectFoundryHub03072025_2'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = evaluate(\n",
    "    data=\"./sample_synthetic_conversations.jsonl\",\n",
    "    evaluators={\n",
    "        \"tool_call_accuracy\": tool_call_accuracy,\n",
    "        \"intent_resolution\": intent_resolution,\n",
    "        \"task_adherence\": task_adherence,\n",
    "        \"response_completeness\": response_completeness,\n",
    "    },\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"RG_NAME\"],\n",
    "    },\n",
    ")\n",
    "response.get(\"studio_url\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiops2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
