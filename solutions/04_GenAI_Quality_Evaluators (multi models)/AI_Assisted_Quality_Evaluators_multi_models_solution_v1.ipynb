{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Response Quality Multi Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up and configure multiple Azure OpenAI model endpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "env_var = {\n",
    "    \"gpt-4o\": {\n",
    "        \"endpoint\": os.environ.get(\"AOAI_GPT4O_ENDPOINT\"),\n",
    "        \"key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    },\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"endpoint\": os.environ.get(\"AOAI_GPT4O_MINI_ENDPOINT\"),\n",
    "        \"key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    },    \n",
    "}\n",
    "\n",
    "ai_project_endpoint=os.environ[\"AI_PROJECT_ENDPOINT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"evaluation_dataset.jsonl\", lines=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate the model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate and run automated evaluations comparing model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import QAEvaluator\n",
    "from model_endpoints import ModelEndpoints\n",
    "import random\n",
    "\n",
    "qa_evaluator = QAEvaluator(model_config)\n",
    "\n",
    "models = [\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\",\n",
    "]\n",
    "\n",
    "path = \"./evaluation_dataset.jsonl\"\n",
    "randomNum = random.randint(1111, 9999)\n",
    "\n",
    "for model in models:\n",
    "    results = evaluate(\n",
    "        evaluation_name=\"04- Eval-Run\" + str(randomNum) + \"-\" + model.title(),\n",
    "        data=path,\n",
    "        target=ModelEndpoints(env_var, model),\n",
    "        evaluators={\n",
    "            \"qa\": qa_evaluator,\n",
    "        },\n",
    "        azure_ai_project=ai_project_endpoint,\n",
    "        evaluator_config={\n",
    "            \"qa\": {\n",
    "                \"column_mapping\": {\n",
    "                    \"query\": \"${data.query}\",\n",
    "                    \"context\": \"${data.context}\",\n",
    "                    \"ground_truth\": \"${data.ground_truth}\",\n",
    "                    \"response\": \"${target.response}\",\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "# Use the URL to inspect the results on the UI.\n",
    "print(f'AI Foundry URL: {results.get(\"studio_url\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results[\"rows\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dryrun2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
