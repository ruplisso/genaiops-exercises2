{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install azure-ai-evaluation\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"RG_NAME\"),\n",
    "    \"project_name\": os.environ.get(\"PROJECT_NAME\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NLP Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BleuScoreEvaluator\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) score is commonly used in natural language processing (NLP) and machine\n",
    "translation. It is widely used in text summarization and text generation use cases. It evaluates how closely the\n",
    "generated text matches the reference text. The BLEU score ranges from 0 to 1, with higher scores indicating\n",
    "better quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Could not import RedTeam. Please install the dependency with `pip install azure-ai-evaluation[redteam]`.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "\n",
    "bleu = BleuScoreEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu_score': 0.22961813530951883, 'bleu_result': 'fail', 'bleu_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "result = bleu(response=\"Tokyo is the capital of Japan.\", ground_truth=\"The capital of Japan is Tokyo.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GleuScoreEvaluator\n",
    "\n",
    "The GLEU (Google-BLEU) score evaluator measures the similarity between generated and reference texts by\n",
    "evaluating n-gram overlap, considering both precision and recall. This balanced evaluation, designed for\n",
    "sentence-level assessment, makes it ideal for detailed analysis of translation quality. GLEU is well-suited for\n",
    "use cases such as machine translation, text summarization, and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GleuScoreEvaluator\n",
    "\n",
    "gleu = GleuScoreEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gleu_score': 0.4090909090909091, 'gleu_result': 'fail', 'gleu_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "result = gleu(response=\"Tokyo is the capital of Japan.\", ground_truth=\"The capital of Japan is Tokyo.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeteorScoreEvaluator\n",
    "\n",
    "The METEOR (Metric for Evaluation of Translation with Explicit Ordering) score grader evaluates generated text by\n",
    "comparing it to reference texts, focusing on precision, recall, and content alignment. It addresses limitations of\n",
    "other metrics like BLEU by considering synonyms, stemming, and paraphrasing. METEOR score considers synonyms and\n",
    "word stems to more accurately capture meaning and language variations. In addition to machine translation and\n",
    "text summarization, paraphrase detection is an optimal use case for the METEOR score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import MeteorScoreEvaluator\n",
    "\n",
    "meteor = MeteorScoreEvaluator(alpha=0.9, beta=3.0, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meteor_score': 0.9067055393586005, 'meteor_result': 'fail', 'meteor_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "result = meteor(response=\"Tokyo is the capital of Japan.\", ground_truth=\"The capital of Japan is Tokyo.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RougeScoreEvaluator\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate automatic\n",
    "summarization and machine translation. It measures the overlap between generated text and reference summaries.\n",
    "ROUGE focuses on recall-oriented measures to assess how well the generated text covers the reference text. Text\n",
    "summarization and document comparison are among optimal use cases for ROUGE, particularly in scenarios where text\n",
    "coherence and relevance are critical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RougeScoreEvaluator, RougeType\n",
    "\n",
    "rouge = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge_precision': 1.0, 'rouge_recall': 1.0, 'rouge_f1_score': 1.0, 'rouge_precision_result': 'pass', 'rouge_recall_result': 'pass', 'rouge_f1_score_result': 'pass', 'rouge_precision_threshold': 0.5, 'rouge_recall_threshold': 0.5, 'rouge_f1_score_threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "result = rouge(response=\"Tokyo is the capital of Japan.\", ground_truth=\"The capital of Japan is Tokyo.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a Dataset using Math Evaluators\n",
    "\n",
    "The code below uses the Evaluate API with BLEU, GLEU, METEOR, and ROUGE evaluators to evaluate the results on a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-28 09:43:09 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 09:43:09 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 09:43:09 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 09:43:09 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-28 09:43:09 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_nt1mrjdt_20250728_094309_692679, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_nt1mrjdt_20250728_094309_692679\\logs.txt\n",
      "[2025-07-28 09:43:09 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_uatb_feu_20250728_094309_698746, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_uatb_feu_20250728_094309_698746\\logs.txt\n",
      "[2025-07-28 09:43:09 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_n7qgux4g_20250728_094309_699743, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_n7qgux4g_20250728_094309_699743\\logs.txt\n",
      "[2025-07-28 09:43:09 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_cx4mrbmq_20250728_094309_698746, log path: C:\\Users\\ruplisso\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_cx4mrbmq_20250728_094309_698746\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 09:43:14 +0200   38844 execution.bulk     INFO     Finished 50 / 50 lines.\n",
      "2025-07-28 09:43:14 +0200   38844 execution.bulk     INFO     Average execution time for completed lines: 0.09 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-07-28 09:43:14 +0200   38844 execution.bulk     INFO     Finished 50 / 50 lines.\n",
      "2025-07-28 09:43:14 +0200   38844 execution.bulk     INFO     Average execution time for completed lines: 0.09 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-07-28 09:43:14 +0200   38844 execution.bulk     INFO     Finished 50 / 50 lines.\n",
      "2025-07-28 09:43:14 +0200   38844 execution.bulk     INFO     Average execution time for completed lines: 0.09 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"data.jsonl\",\n",
    "    evaluators={\n",
    "        \"bleu\": bleu,\n",
    "        \"gleu\": gleu,\n",
    "        \"meteor\": meteor,\n",
    "        \"rouge\": rouge,\n",
    "    },\n",
    "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
    "    azure_ai_project=azure_ai_project,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaiops2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
