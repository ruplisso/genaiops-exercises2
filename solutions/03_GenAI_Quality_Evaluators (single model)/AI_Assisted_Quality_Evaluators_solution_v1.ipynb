{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Response Quality with RelevanceEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevance': 4.0, 'gpt_relevance': 4.0, 'relevance_reason': 'The response directly and accurately answers the query by stating that Paris is the capital of France. It fully satisfies the question without omissions or additional context.', 'relevance_result': 'pass', 'relevance_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "\n",
    "# Evaluate a single query-response pair\n",
    "result = relevance_evaluator(\n",
    "    query=\"What is the capital of France?\",\n",
    "    response=\"The capital of France is Paris.\",\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Response Quality with CoherenceEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coherence': 4.0, 'gpt_coherence': 4.0, 'coherence_reason': 'The response is coherent and directly answers the query, but it is minimal and does not showcase advanced organization or flow.', 'coherence_result': 'pass', 'coherence_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import CoherenceEvaluator\n",
    "\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "\n",
    "result = coherence_evaluator(\n",
    "    query=\"What's the capital of France?\", \n",
    "    response=\"Paris.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Response Quality with FluencyEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fluency': 1.0, 'gpt_fluency': 1.0, 'fluency_reason': 'The RESPONSE is a single word without any grammatical structure or context, making it incomprehensible and indicative of minimal command of the language.', 'fluency_result': 'fail', 'fluency_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import FluencyEvaluator\n",
    "\n",
    "fluency_evaluator = FluencyEvaluator(model_config)\n",
    "\n",
    "result = fluency_evaluator(\n",
    "    response=\"Paris.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Response Quality with GroundednessEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'groundedness': 3.0, 'gpt_groundedness': 3.0, 'groundedness_reason': 'The RESPONSE provides the correct name of the discoverer but includes an incorrect year, which makes it an attempt to respond but with erroneous details.', 'groundedness_result': 'pass', 'groundedness_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import GroundednessEvaluator\n",
    "\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "\n",
    "result = groundedness_evaluator(\n",
    "    query=\"Who discovered penicillin?\",\n",
    "    context=\"Alexander Fleming discovered penicillin in 1928 while working at St. Mary's Hospital in London.\",\n",
    "    response=\"Alexander Fleming discovered penicillin in 1938.\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating custom evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code-based evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function-based evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response_length': 13}\n"
     ]
    }
   ],
   "source": [
    "# Custom evaluator function to calculate response length\n",
    "def response_length_evaluator(response, **kwargs):\n",
    "    return {\"response_length\": len(response)}\n",
    "\n",
    "# Example usage\n",
    "result = response_length_evaluator(response=\"Hello, world!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-based evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contains_blocked_word': True}\n"
     ]
    }
   ],
   "source": [
    "# Custom class-based evaluator to check for blocked words\n",
    "class BlocklistEvaluator:\n",
    "    def __init__(self, blocklist):\n",
    "        self.blocklist = blocklist\n",
    "\n",
    "    def __call__(self, *, response: str, **kwargs):\n",
    "        contains_blocked_word = any(word in response for word in self.blocklist)\n",
    "        return {\"contains_blocked_word\": contains_blocked_word}\n",
    "    \n",
    "# Example usage\n",
    "blocklist_evaluator = BlocklistEvaluator(blocklist=[\"bad\", \"evil\", \"worst\"])\n",
    "result = blocklist_evaluator(response=\"This is the worst response ever!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt-based evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpfulness evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'helpfulness': 1.0, 'helpfulness_reason': 'The RESPONSE is entirely unhelpful as it does not address the philosophical question or provide any useful information related to the CONTEXT or QUERY.'}\n"
     ]
    }
   ],
   "source": [
    "from helpfulness import HelpfulnessEvaluator\n",
    "\n",
    "helpfulness_evaluator = HelpfulnessEvaluator(model_config)\n",
    "\n",
    "helpfulness_score = helpfulness_evaluator(\n",
    "    query=\"What's the meaning of life?\", \n",
    "    context=\"Arthur Schopenhauer was the first to explicitly ask the question, in an essay entitled 'Character'.\", \n",
    "    response=\"The answer is 42.\"\n",
    ")\n",
    "print(helpfulness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON accuracy evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'json_schema': 0.5, 'json_schema_reason': 'The JSON output is partially correct but contains a missing required field (\"companyName\") in \"companyInfo\". All other fields, types, and formats are correct.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from json_schema import JSONSchemaEvaluator\n",
    "\n",
    "# Load jsons/example.jsonl file here\n",
    "example_json_schema = json.load(open('jsons/example_schema.json', 'r'))\n",
    "\n",
    "# Example JSON object\n",
    "sample_json_data = json.load(open('jsons/poor_output.json', 'r'))\n",
    "\n",
    "json_schema_evaluator = JSONSchemaEvaluator(model_config)\n",
    "json_schema_score = json_schema_evaluator(json_output=sample_json_data, schema=example_json_schema)\n",
    "print(json_schema_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.ai.evaluation import evaluate, RetrievalEvaluator, RelevanceEvaluator, FluencyEvaluator, CoherenceEvaluator, GroundednessEvaluator\n",
    "#from helpfulness import HelpfulnessEvaluator\n",
    "from pprint import pprint\n",
    "from model_endpoint import ModelEndpoint\n",
    "\n",
    "ai_project_endpoint=os.environ[\"AI_PROJECT_ENDPOINT\"]\n",
    "\n",
    "# Define your evaluators\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "fluency_evaluator = FluencyEvaluator(model_config)\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "retrieval_evaluator = RetrievalEvaluator(model_config)\n",
    "helpfulness_evaluator = HelpfulnessEvaluator(model_config)\n",
    "\n",
    "# Evaluate the dataset\n",
    "result = evaluate(\n",
    "    evaluation_name=\"Quality Evaluators - Single Model\",\n",
    "    data=\"evaluation_dataset.jsonl\",\n",
    "    target=ModelEndpoint(model_config),\n",
    "    evaluators={\n",
    "        # Performance and quality evaluators (AI-assisted)\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "        \"groundedness\": groundedness_evaluator,\n",
    "        \"retrieval\": retrieval_evaluator,\n",
    "        # Custom evaluators (code and prompt based)\n",
    "        \"helpfulness\": helpfulness_evaluator,\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\"response\": \"${target.response}\", \"context\": \"${data.context}\", \"query\": \"${data.query}\"}\n",
    "        },\n",
    "        \"coherence\": {\n",
    "            \"column_mapping\": {\"response\": \"${target.response}\", \"query\": \"${data.query}\"}\n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"column_mapping\": {\"response\": \"${target.response}\"}\n",
    "        },\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\"response\": \"${target.response}\", \"context\": \"${data.context}\", \"query\": \"${data.query}\"}\n",
    "        },\n",
    "        \"retrieval\": {\n",
    "            \"column_mapping\": {\"context\": \"${data.context}\", \"query\": \"${data.query}\"}\n",
    "        },\n",
    "        \"helpfulness\": {\n",
    "            \"column_mapping\": {\"response\": \"${target.response}\", \"context\": \"${data.context}\", \"query\": \"${data.query}\"}\n",
    "        },\n",
    "    },\n",
    "    azure_ai_project=ai_project_endpoint,\n",
    "    output_path=\"./evaluation_results.json\",\n",
    ")\n",
    "\n",
    "print(f'AI Foundry URL: {result.get(\"studio_url\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.query</th>\n",
       "      <th>inputs.context</th>\n",
       "      <th>inputs.ground_truth</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>outputs.query</th>\n",
       "      <th>outputs.response</th>\n",
       "      <th>outputs.relevance.relevance</th>\n",
       "      <th>outputs.relevance.gpt_relevance</th>\n",
       "      <th>outputs.relevance.relevance_reason</th>\n",
       "      <th>outputs.relevance.relevance_result</th>\n",
       "      <th>...</th>\n",
       "      <th>outputs.groundedness.groundedness_result</th>\n",
       "      <th>outputs.groundedness.groundedness_threshold</th>\n",
       "      <th>outputs.retrieval.retrieval</th>\n",
       "      <th>outputs.retrieval.gpt_retrieval</th>\n",
       "      <th>outputs.retrieval.retrieval_reason</th>\n",
       "      <th>outputs.retrieval.retrieval_result</th>\n",
       "      <th>outputs.retrieval.retrieval_threshold</th>\n",
       "      <th>outputs.helpfulness.helpfulness</th>\n",
       "      <th>outputs.helpfulness.helpfulness_reason</th>\n",
       "      <th>line_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What event started on July 28, 1914?</td>\n",
       "      <td>It involved multiple countries and lasted unti...</td>\n",
       "      <td>World War I</td>\n",
       "      <td>0</td>\n",
       "      <td>What event started on July 28, 1914?</td>\n",
       "      <td>On **July 28, 1914**, **World War I** official...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response directly answers the query by sta...</td>\n",
       "      <td>pass</td>\n",
       "      <td>...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The context is partially relevant as it hints ...</td>\n",
       "      <td>fail</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The RESPONSE is entirely helpful as it accurat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the first person to walk on the moon?</td>\n",
       "      <td>The event occurred during the Apollo 11 missio...</td>\n",
       "      <td>Neil Armstrong</td>\n",
       "      <td>1</td>\n",
       "      <td>Who was the first person to walk on the moon?</td>\n",
       "      <td>The first person to walk on the moon was **Nei...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response directly answers the query by ide...</td>\n",
       "      <td>pass</td>\n",
       "      <td>...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The context is partially relevant as it mentio...</td>\n",
       "      <td>fail</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The RESPONSE fully answers the QUERY with accu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was the significance of the year 1776 in ...</td>\n",
       "      <td>A key document was signed declaring independen...</td>\n",
       "      <td>The Declaration of Independence</td>\n",
       "      <td>2</td>\n",
       "      <td>What was the significance of the year 1776 in ...</td>\n",
       "      <td>The year 1776 is one of the most significant i...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response thoroughly addresses the query by...</td>\n",
       "      <td>pass</td>\n",
       "      <td>...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context fully addresses the query and plac...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The RESPONSE is entirely helpful as it fully a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which wall fell in 1989, symbolizing the end o...</td>\n",
       "      <td>It divided a German city into East and West.</td>\n",
       "      <td>The Berlin Wall</td>\n",
       "      <td>3</td>\n",
       "      <td>Which wall fell in 1989, symbolizing the end o...</td>\n",
       "      <td>The **Berlin Wall** fell in 1989, symbolizing ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response directly answers the query by ide...</td>\n",
       "      <td>pass</td>\n",
       "      <td>...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The context is partially relevant but does not...</td>\n",
       "      <td>fail</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The RESPONSE fully answers the QUERY, provides...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What ancient city was buried by the eruption o...</td>\n",
       "      <td>The city's ruins were rediscovered in the 18th...</td>\n",
       "      <td>Pompeii</td>\n",
       "      <td>4</td>\n",
       "      <td>What ancient city was buried by the eruption o...</td>\n",
       "      <td>The ancient city of **Pompeii** was famously b...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response directly answers the query by ide...</td>\n",
       "      <td>pass</td>\n",
       "      <td>...</td>\n",
       "      <td>fail</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The context is partially relevant but does not...</td>\n",
       "      <td>fail</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The RESPONSE fully answers the QUERY and provi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Who was the British Prime Minister during Worl...</td>\n",
       "      <td>He is famous for his leadership and speeches, ...</td>\n",
       "      <td>Winston Churchill</td>\n",
       "      <td>5</td>\n",
       "      <td>Who was the British Prime Minister during Worl...</td>\n",
       "      <td>The British Prime Minister during most of Worl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response fully answers the query by identi...</td>\n",
       "      <td>pass</td>\n",
       "      <td>...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The context is partially relevant as it hints ...</td>\n",
       "      <td>fail</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The RESPONSE is entirely helpful as it accurat...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What was the name of the ship that sank on its...</td>\n",
       "      <td>It was deemed 'unsinkable' before it hit an ic...</td>\n",
       "      <td>RMS Titanic</td>\n",
       "      <td>6</td>\n",
       "      <td>What was the name of the ship that sank on its...</td>\n",
       "      <td>The ship that sank on its maiden voyage in 191...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response directly answers the query by nam...</td>\n",
       "      <td>pass</td>\n",
       "      <td>...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The context is partially relevant as it provid...</td>\n",
       "      <td>fail</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The RESPONSE is entirely helpful as it accurat...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Which empire was ruled by Genghis Khan?</td>\n",
       "      <td>This empire became the largest contiguous land...</td>\n",
       "      <td>The Mongol Empire</td>\n",
       "      <td>7</td>\n",
       "      <td>Which empire was ruled by Genghis Khan?</td>\n",
       "      <td>Genghis Khan ruled the **Mongol Empire**, whic...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response directly answers the query by ide...</td>\n",
       "      <td>pass</td>\n",
       "      <td>...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The context is partially relevant to the query...</td>\n",
       "      <td>fail</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The RESPONSE is entirely helpful as it accurat...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What was the primary cause of the American Civ...</td>\n",
       "      <td>The conflict between the Northern and Southern...</td>\n",
       "      <td>Slavery</td>\n",
       "      <td>8</td>\n",
       "      <td>What was the primary cause of the American Civ...</td>\n",
       "      <td>The primary cause of the American Civil War wa...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response thoroughly addresses the query by...</td>\n",
       "      <td>pass</td>\n",
       "      <td>...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The context is highly relevant to the query, d...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The RESPONSE is entirely helpful as it compreh...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Which ancient wonder was located in Egypt and ...</td>\n",
       "      <td>It is the only one of the Seven Wonders of the...</td>\n",
       "      <td>The Great Pyramid of Giza</td>\n",
       "      <td>9</td>\n",
       "      <td>Which ancient wonder was located in Egypt and ...</td>\n",
       "      <td>The ancient wonder located in Egypt that serve...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response fully answers the query by identi...</td>\n",
       "      <td>pass</td>\n",
       "      <td>...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The context is partially relevant as it indire...</td>\n",
       "      <td>fail</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The RESPONSE fully answers the QUERY, providin...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        inputs.query  \\\n",
       "0               What event started on July 28, 1914?   \n",
       "1      Who was the first person to walk on the moon?   \n",
       "2  What was the significance of the year 1776 in ...   \n",
       "3  Which wall fell in 1989, symbolizing the end o...   \n",
       "4  What ancient city was buried by the eruption o...   \n",
       "5  Who was the British Prime Minister during Worl...   \n",
       "6  What was the name of the ship that sank on its...   \n",
       "7            Which empire was ruled by Genghis Khan?   \n",
       "8  What was the primary cause of the American Civ...   \n",
       "9  Which ancient wonder was located in Egypt and ...   \n",
       "\n",
       "                                      inputs.context  \\\n",
       "0  It involved multiple countries and lasted unti...   \n",
       "1  The event occurred during the Apollo 11 missio...   \n",
       "2  A key document was signed declaring independen...   \n",
       "3       It divided a German city into East and West.   \n",
       "4  The city's ruins were rediscovered in the 18th...   \n",
       "5  He is famous for his leadership and speeches, ...   \n",
       "6  It was deemed 'unsinkable' before it hit an ic...   \n",
       "7  This empire became the largest contiguous land...   \n",
       "8  The conflict between the Northern and Southern...   \n",
       "9  It is the only one of the Seven Wonders of the...   \n",
       "\n",
       "               inputs.ground_truth  inputs.line_number  \\\n",
       "0                      World War I                   0   \n",
       "1                   Neil Armstrong                   1   \n",
       "2  The Declaration of Independence                   2   \n",
       "3                  The Berlin Wall                   3   \n",
       "4                          Pompeii                   4   \n",
       "5                Winston Churchill                   5   \n",
       "6                      RMS Titanic                   6   \n",
       "7                The Mongol Empire                   7   \n",
       "8                          Slavery                   8   \n",
       "9        The Great Pyramid of Giza                   9   \n",
       "\n",
       "                                       outputs.query  \\\n",
       "0               What event started on July 28, 1914?   \n",
       "1      Who was the first person to walk on the moon?   \n",
       "2  What was the significance of the year 1776 in ...   \n",
       "3  Which wall fell in 1989, symbolizing the end o...   \n",
       "4  What ancient city was buried by the eruption o...   \n",
       "5  Who was the British Prime Minister during Worl...   \n",
       "6  What was the name of the ship that sank on its...   \n",
       "7            Which empire was ruled by Genghis Khan?   \n",
       "8  What was the primary cause of the American Civ...   \n",
       "9  Which ancient wonder was located in Egypt and ...   \n",
       "\n",
       "                                    outputs.response  \\\n",
       "0  On **July 28, 1914**, **World War I** official...   \n",
       "1  The first person to walk on the moon was **Nei...   \n",
       "2  The year 1776 is one of the most significant i...   \n",
       "3  The **Berlin Wall** fell in 1989, symbolizing ...   \n",
       "4  The ancient city of **Pompeii** was famously b...   \n",
       "5  The British Prime Minister during most of Worl...   \n",
       "6  The ship that sank on its maiden voyage in 191...   \n",
       "7  Genghis Khan ruled the **Mongol Empire**, whic...   \n",
       "8  The primary cause of the American Civil War wa...   \n",
       "9  The ancient wonder located in Egypt that serve...   \n",
       "\n",
       "   outputs.relevance.relevance  outputs.relevance.gpt_relevance  \\\n",
       "0                          5.0                              5.0   \n",
       "1                          5.0                              5.0   \n",
       "2                          5.0                              5.0   \n",
       "3                          5.0                              5.0   \n",
       "4                          5.0                              5.0   \n",
       "5                          5.0                              5.0   \n",
       "6                          5.0                              5.0   \n",
       "7                          5.0                              5.0   \n",
       "8                          5.0                              5.0   \n",
       "9                          5.0                              5.0   \n",
       "\n",
       "                  outputs.relevance.relevance_reason  \\\n",
       "0  The response directly answers the query by sta...   \n",
       "1  The response directly answers the query by ide...   \n",
       "2  The response thoroughly addresses the query by...   \n",
       "3  The response directly answers the query by ide...   \n",
       "4  The response directly answers the query by ide...   \n",
       "5  The response fully answers the query by identi...   \n",
       "6  The response directly answers the query by nam...   \n",
       "7  The response directly answers the query by ide...   \n",
       "8  The response thoroughly addresses the query by...   \n",
       "9  The response fully answers the query by identi...   \n",
       "\n",
       "  outputs.relevance.relevance_result  ...  \\\n",
       "0                               pass  ...   \n",
       "1                               pass  ...   \n",
       "2                               pass  ...   \n",
       "3                               pass  ...   \n",
       "4                               pass  ...   \n",
       "5                               pass  ...   \n",
       "6                               pass  ...   \n",
       "7                               pass  ...   \n",
       "8                               pass  ...   \n",
       "9                               pass  ...   \n",
       "\n",
       "   outputs.groundedness.groundedness_result  \\\n",
       "0                                      pass   \n",
       "1                                      pass   \n",
       "2                                      pass   \n",
       "3                                      pass   \n",
       "4                                      fail   \n",
       "5                                      pass   \n",
       "6                                      pass   \n",
       "7                                      pass   \n",
       "8                                      pass   \n",
       "9                                      pass   \n",
       "\n",
       "   outputs.groundedness.groundedness_threshold  outputs.retrieval.retrieval  \\\n",
       "0                                            3                          2.0   \n",
       "1                                            3                          2.0   \n",
       "2                                            3                          5.0   \n",
       "3                                            3                          2.0   \n",
       "4                                            3                          2.0   \n",
       "5                                            3                          2.0   \n",
       "6                                            3                          2.0   \n",
       "7                                            3                          2.0   \n",
       "8                                            3                          5.0   \n",
       "9                                            3                          2.0   \n",
       "\n",
       "  outputs.retrieval.gpt_retrieval  \\\n",
       "0                             2.0   \n",
       "1                             2.0   \n",
       "2                             5.0   \n",
       "3                             2.0   \n",
       "4                             2.0   \n",
       "5                             2.0   \n",
       "6                             2.0   \n",
       "7                             2.0   \n",
       "8                             5.0   \n",
       "9                             2.0   \n",
       "\n",
       "                  outputs.retrieval.retrieval_reason  \\\n",
       "0  The context is partially relevant as it hints ...   \n",
       "1  The context is partially relevant as it mentio...   \n",
       "2  The context fully addresses the query and plac...   \n",
       "3  The context is partially relevant but does not...   \n",
       "4  The context is partially relevant but does not...   \n",
       "5  The context is partially relevant as it hints ...   \n",
       "6  The context is partially relevant as it provid...   \n",
       "7  The context is partially relevant to the query...   \n",
       "8  The context is highly relevant to the query, d...   \n",
       "9  The context is partially relevant as it indire...   \n",
       "\n",
       "   outputs.retrieval.retrieval_result  outputs.retrieval.retrieval_threshold  \\\n",
       "0                                fail                                      3   \n",
       "1                                fail                                      3   \n",
       "2                                pass                                      3   \n",
       "3                                fail                                      3   \n",
       "4                                fail                                      3   \n",
       "5                                fail                                      3   \n",
       "6                                fail                                      3   \n",
       "7                                fail                                      3   \n",
       "8                                pass                                      3   \n",
       "9                                fail                                      3   \n",
       "\n",
       "   outputs.helpfulness.helpfulness  \\\n",
       "0                              5.0   \n",
       "1                              5.0   \n",
       "2                              5.0   \n",
       "3                              5.0   \n",
       "4                              5.0   \n",
       "5                              5.0   \n",
       "6                              5.0   \n",
       "7                              5.0   \n",
       "8                              5.0   \n",
       "9                              5.0   \n",
       "\n",
       "              outputs.helpfulness.helpfulness_reason line_number  \n",
       "0  The RESPONSE is entirely helpful as it accurat...           0  \n",
       "1  The RESPONSE fully answers the QUERY with accu...           1  \n",
       "2  The RESPONSE is entirely helpful as it fully a...           2  \n",
       "3  The RESPONSE fully answers the QUERY, provides...           3  \n",
       "4  The RESPONSE fully answers the QUERY and provi...           4  \n",
       "5  The RESPONSE is entirely helpful as it accurat...           5  \n",
       "6  The RESPONSE is entirely helpful as it accurat...           6  \n",
       "7  The RESPONSE is entirely helpful as it accurat...           7  \n",
       "8  The RESPONSE is entirely helpful as it compreh...           8  \n",
       "9  The RESPONSE fully answers the QUERY, providin...           9  \n",
       "\n",
       "[10 rows x 34 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result[\"rows\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Retrieval Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg@3': 0.31075932533963707,\n",
       " 'xdcg@3': 39.285714285714285,\n",
       " 'fidelity': 0.39285714285714285,\n",
       " 'top1_relevance': 2,\n",
       " 'top3_max_relevance': 3,\n",
       " 'holes': 2,\n",
       " 'holes_ratio': 0.4,\n",
       " 'total_retrieved_documents': 5,\n",
       " 'total_ground_truth_documents': 5,\n",
       " 'ndcg@3_result': 'fail',\n",
       " 'ndcg@3_threshold': 0.5,\n",
       " 'ndcg@3_higher_is_better': True,\n",
       " 'xdcg@3_result': 'fail',\n",
       " 'xdcg@3_threshold': 50.0,\n",
       " 'xdcg@3_higher_is_better': True,\n",
       " 'fidelity_result': 'fail',\n",
       " 'fidelity_threshold': 0.5,\n",
       " 'fidelity_higher_is_better': True,\n",
       " 'top1_relevance_result': 'fail',\n",
       " 'top1_relevance_threshold': 50.0,\n",
       " 'top1_relevance_higher_is_better': True,\n",
       " 'top3_max_relevance_result': 'fail',\n",
       " 'top3_max_relevance_threshold': 50.0,\n",
       " 'top3_max_relevance_higher_is_better': True,\n",
       " 'holes_result': 'fail',\n",
       " 'holes_threshold': 0,\n",
       " 'holes_higher_is_better': False,\n",
       " 'holes_ratio_result': 'fail',\n",
       " 'holes_ratio_threshold': 0,\n",
       " 'holes_ratio_higher_is_better': False,\n",
       " 'total_retrieved_documents_result': 'fail',\n",
       " 'total_retrieved_documents_threshold': 50,\n",
       " 'total_retrieved_documents_higher_is_better': True,\n",
       " 'total_ground_truth_documents_result': 'fail',\n",
       " 'total_ground_truth_documents_threshold': 50,\n",
       " 'total_ground_truth_documents_higher_is_better': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.evaluation import DocumentRetrievalEvaluator\n",
    "\n",
    "# these query_relevance_label are given by your human- or LLM-judges.\n",
    "retrieval_ground_truth = [\n",
    "    {\n",
    "        \"document_id\": \"1\",\n",
    "        \"query_relevance_label\": 4\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"2\",\n",
    "        \"query_relevance_label\": 2\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"3\",\n",
    "        \"query_relevance_label\": 3\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"4\",\n",
    "        \"query_relevance_label\": 1\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"5\",\n",
    "        \"query_relevance_label\": 0\n",
    "    },\n",
    "]\n",
    "# the min and max of the label scores are inputs to document retrieval evaluator\n",
    "ground_truth_label_min = 0\n",
    "ground_truth_label_max = 4\n",
    "\n",
    "# these relevance scores come from your search retrieval system\n",
    "retrieved_documents = [\n",
    "    {\n",
    "        \"document_id\": \"2\",\n",
    "        \"relevance_score\": 45.1\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"6\",\n",
    "        \"relevance_score\": 35.8\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"3\",\n",
    "        \"relevance_score\": 29.2\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"5\",\n",
    "        \"relevance_score\": 25.4\n",
    "    },\n",
    "    {\n",
    "        \"document_id\": \"7\",\n",
    "        \"relevance_score\": 18.8\n",
    "    },\n",
    "]\n",
    "\n",
    "document_retrieval_evaluator = DocumentRetrievalEvaluator(\n",
    "    ground_truth_label_min=ground_truth_label_min, \n",
    "    ground_truth_label_max=ground_truth_label_max,\n",
    "    ndcg_threshold = 0.5,\n",
    "    xdcg_threshold = 50.0,\n",
    "    fidelity_threshold = 0.5,\n",
    "    top1_relevance_threshold = 50.0,\n",
    "    top3_max_relevance_threshold = 50.0,\n",
    "    total_retrieved_documents_threshold = 50,\n",
    "    total_ground_truth_documents_threshold = 50\n",
    ")\n",
    "document_retrieval_evaluator(retrieval_ground_truth=retrieval_ground_truth, retrieved_documents=retrieved_documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dryrun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
