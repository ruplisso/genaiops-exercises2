{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Goal of the Exercise  \n",
    "\n",
    "In this exercise, you'll explore how to effectively evaluate the quality of AI-generated content using **multiple AI models simultaneously**. You can work with a **Composite evaluator** to measure performance across different built-in AI-assisted evaluation methods.\n",
    "\n",
    "Through this hands-on activity, you'll learn how to:\n",
    "\n",
    "- Set up and configure multiple Azure OpenAI model endpoints.\n",
    "- Load, preprocess, and prepare datasets for evaluation tasks.\n",
    "- Run automated evaluations and systematically compare model outputs.\n",
    "- Analyze results to identify which AI models perform best on specific tasks.\n",
    "\n",
    "By the end of this exercise, you'll gain practical experience in multi-model evaluations, enabling you to make informed decisions when selecting or optimizing AI models for your specific use cases.\n",
    "\n",
    "### Link to documentation\n",
    "\n",
    "https://github.com/Azure-Samples/azureai-samples/blob/main/scenarios/evaluate/Supported_Evaluation_Targets/Evaluate_Base_Model_Endpoint/Evaluate_Base_Model_Endpoint.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff60b82",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Compare Generative AI models against same test data on AI-assisted quality evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Like the former exercise (03_GenAI_Quality_Evaluators) you'll have to measure performance\n",
    "# of GenAI model against a test data\n",
    "# The difference is that this time you will have to measure the performance of two (or more) models\n",
    "# and compare them such as gpt-4o vs. gpt-4o-mini\n",
    "\n",
    "# You can use the model_endpoints.py file to get the endpoints of the models\n",
    "\n",
    "# Optional: you can still evaluate the performance of a single model but with different hyperparameters\n",
    "# (e.g. temperature, max tokens, seed, etc.) to see how they affect the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
