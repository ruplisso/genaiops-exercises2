{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Goal of the Exercise\n",
    "In this exercise, you'll explore how to effectively evaluate text-based AI outputs using common **NLP evaluators**, such as **BLEU**, **GLEU**, **METEOR**, and **ROUGE**. Your task is to implement each evaluator, measure the similarity between AI-generated responses and reference texts, and assess the quality of text-generation models.\n",
    "\n",
    "While these NLP evaluators might not be the right metrics to evaluate the performance of your Generative AI applications, it gives you a flavor of what looks like evaluation metrics of traditional Natural Language Processing (NLP) tasks.\n",
    "\n",
    "This hands-on practice will help you understand:\n",
    "\n",
    "- How NLP evaluation metrics are practically applied.\n",
    "- The nuances between different evaluators.\n",
    "- Best practices to objectively measure text-generation accuracy and quality.\n",
    "\n",
    "Here's a concise table comparing **BLEU**, **GLEU**, **METEOR**, and **ROUGE** evaluators clearly:\n",
    "\n",
    "| Criteria                 | BLEU                                       | GLEU                                       | METEOR                                       | ROUGE                                         |\n",
    "|--------------------------|--------------------------------------------|--------------------------------------------|----------------------------------------------|-----------------------------------------------|\n",
    "| **Primary usage**        | Machine translation                         | Grammar correction / translation fluency   | Machine translation (fluency, semantics)     | Text summarization & information extraction   |\n",
    "| **Evaluation method**    | Precision-based with brevity penalty        | Precision and recall equally weighted      | Precision, recall, alignment, synonyms, stemming | Recall-focused (but also precision, F1-score)|\n",
    "| **N-gram consideration** | Overlapping n-grams (Precision only)        | Overlapping n-grams (Precision & Recall)   | Flexible alignment with synonym/stemming support | Overlapping n-grams (Recall-focused)         |\n",
    "| **Main strengths**       | Simple, widely-used, computationally efficient | Better correlation with grammatical correctness | Strong correlation with human judgment, semantic similarity | Excellent for summarization tasks, recall-oriented |\n",
    "| **Main weaknesses**      | Ignores recall and fluency; limited semantic awareness | Less standardized in translation evaluations | More complex, slower to compute              | Less suitable for translation evaluation (focuses mainly on recall) |\n",
    "\n",
    "---\n",
    "\n",
    "### Quick insights on when to choose each metric:\n",
    "\n",
    "- **BLEU**: For traditional machine translation benchmarks prioritizing simplicity and precision.\n",
    "- **GLEU**: For grammatical correctness or fluency-oriented evaluations requiring balanced precision/recall.\n",
    "- **METEOR**: When semantic similarity, human judgment alignment, and nuanced evaluation (synonyms/stemming) matter significantly.\n",
    "- **ROUGE**: Primarily for text summarization or extractive tasks emphasizing recall (content coverage) over precision.\n",
    "\n",
    "### Links to Documentation:\n",
    "https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/textual-similarity-evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"RG_NAME\"),\n",
    "    \"project_name\": os.environ.get(\"PROJECT_NAME\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NLP Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BleuScoreEvaluator\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) score is commonly used in natural language processing (NLP) and machine\n",
    "translation. It is widely used in text summarization and text generation use cases. It evaluates how closely the\n",
    "generated text matches the reference text. The BLEU score ranges from 0 to 1, with higher scores indicating\n",
    "better quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dfc796",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the BLEU Score Evaluator\n",
    "Fill in the missing code to initialize and use the `BleuScoreEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate BleuScoreEvaluator and use it to evaluate a response vs. a reference (\"ground truth\") text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GleuScoreEvaluator\n",
    "\n",
    "The GLEU (Google-BLEU) score evaluator measures the similarity between generated and reference texts by\n",
    "evaluating n-gram overlap, considering both precision and recall. This balanced evaluation, designed for\n",
    "sentence-level assessment, makes it ideal for detailed analysis of translation quality. GLEU is well-suited for\n",
    "use cases such as machine translation, text summarization, and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa5969",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the GLEU Score Evaluator\n",
    "Complete the code below to correctly instantiate and apply the `GleuScoreEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate GleuScoreEvaluator and use it to evaluate a response vs. a reference (\"ground truth\") text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeteorScoreEvaluator\n",
    "\n",
    "The METEOR (Metric for Evaluation of Translation with Explicit Ordering) score grader evaluates generated text by\n",
    "comparing it to reference texts, focusing on precision, recall, and content alignment. It addresses limitations of\n",
    "other metrics like BLEU by considering synonyms, stemming, and paraphrasing. METEOR score considers synonyms and\n",
    "word stems to more accurately capture meaning and language variations. In addition to machine translation and\n",
    "text summarization, paraphrase detection is an optimal use case for the METEOR score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8b253",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the METEOR Score Evaluator\n",
    "Modify the following cell to properly initialize and evaluate using `MeteorScoreEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate MeteorScoreEvaluator and use it to evaluate a response vs. a reference (\"ground truth\") text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RougeScoreEvaluator\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate automatic\n",
    "summarization and machine translation. It measures the overlap between generated text and reference summaries.\n",
    "ROUGE focuses on recall-oriented measures to assess how well the generated text covers the reference text. Text\n",
    "summarization and document comparison are among optimal use cases for ROUGE, particularly in scenarios where text\n",
    "coherence and relevance are critical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ec16c",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Implement the ROUGE Score Evaluator\n",
    "Update the code below to correctly use the `RougeScoreEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate RougeScoreEvaluator and use it to evaluate a response vs. a reference (\"ground truth\") text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate test data using these NLP evaluators\n",
    "\n",
    "The code below uses the Evaluate API with BLEU, GLEU, METEOR, and ROUGE evaluators to evaluate the results on a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf10c9",
   "metadata": {},
   "source": [
    "### ðŸ”§ Task: Evaluate a Dataset\n",
    "Modify the following code to run evaluations using the evaluators you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "ai_project_endpoint=os.environ[\"AI_PROJECT_ENDPOINT\"]\n",
    "\n",
    "# TODO: Call evaluate function and run NLP evaluators on the provided test data data.jsonl\n",
    "# Optional: Export the evaluation results to your Azure AI Foundry project so - do not forget to be logged in Azure - azd auth login\n",
    "# you can then visualize them in the Azure AI Foundry Portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
